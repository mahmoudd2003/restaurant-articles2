# app.py (Ù†Ø³Ø®Ø© Ù…Ø³ØªÙ‚Ù„Ø© ÙˆÙ…ÙØµÙ„Ø­Ø©)
# -*- coding: utf-8 -*-
# --- ØªØ«Ø¨ÙŠØª Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¹Ù„Ù‰ sys.path Ù„Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ---
import os, sys
ROOT = os.path.dirname(os.path.abspath(__file__))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
# ---------------------------------------------------------------------

import os, json, math, logging, hashlib, time, unicodedata
from datetime import datetime
from typing import Any, Dict, List, Optional

import streamlit as st

# =============== Ø¥Ø¹Ø¯Ø§Ø¯ Ø¹Ø§Ù… ===============
st.set_page_config(page_title="Ù…ÙˆÙ„Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù…", page_icon="ğŸ½ï¸", layout="wide")
os.makedirs("data", exist_ok=True)
os.makedirs("logs", exist_ok=True)

logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s | %(levelname)s | app | %(message)s",
)
logger = logging.getLogger("app")

# =============== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ===============
def slugify(name: str) -> str:
    s = ''.join(c for c in unicodedata.normalize('NFKD', name or "") if not unicodedata.combining(c))
    import re as _re
    s = _re.sub(r'\W+', '-', s).strip('-').lower()
    return s or "item"

def _normalize_lines(s: str) -> List[str]:
    return [ln.strip() for ln in (s or "").splitlines() if ln.strip()]

# â€”â€” ÙƒØ§Ø´ LLM Ø¨Ø³ÙŠØ· Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ù„Ø³Ø© â€”â€” 
class SimpleLLMCache:
    def __init__(self, enabled=True, ttl_hours=24):
        self.enabled = enabled
        self.ttl = ttl_hours * 3600
        if "llm_cache_store" not in st.session_state:
            st.session_state["llm_cache_store"] = {}

    def _key(self, payload: Dict[str, Any]) -> str:
        raw = json.dumps(payload, ensure_ascii=False, sort_keys=True).encode("utf-8")
        return hashlib.sha256(raw).hexdigest()

    def get(self, payload: Dict[str, Any]) -> Optional[str]:
        if not self.enabled: return None
        key = self._key(payload)
        item = st.session_state["llm_cache_store"].get(key)
        if not item: return None
        ts, val = item
        if time.time() - ts > self.ttl:
            st.session_state["llm_cache_store"].pop(key, None)
            return None
        return val

    def set(self, payload: Dict[str, Any], value: str):
        if not self.enabled: return
        key = self._key(payload)
        st.session_state["llm_cache_store"][key] = (time.time(), value)

# â€”â€” ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ø´ â€”â€” 
if "llm_cache" not in st.session_state:
    st.session_state["llm_cache"] = SimpleLLMCache(enabled=True, ttl_hours=24)

# â€”â€” OpenAI Ø¹Ù…ÙŠÙ„ Ù…Ø¨Ø³Ø· â€”â€” 
def _has_api_key() -> bool:
    try:
        return bool((hasattr(st, "secrets") and st.secrets.get("OPENAI_API_KEY")) or os.getenv("OPENAI_API_KEY"))
    except Exception:
        return bool(os.getenv("OPENAI_API_KEY"))

def _get_openai_client():
    # Ù„Ø§ Ù†ÙÙ†Ø´Ø¦ dependency Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©
    from openai import OpenAI
    api_key = (st.secrets.get("OPENAI_API_KEY") if hasattr(st, "secrets") else None) or os.getenv("OPENAI_API_KEY")
    return OpenAI(api_key=api_key)

def chat_complete_cached(messages: List[Dict[str, str]], model: str, max_tokens=2000, temperature=0.7) -> str:
    payload = {"messages": messages, "model": model, "max_tokens": max_tokens, "temperature": temperature}
    cache = st.session_state["llm_cache"]
    hit = cache.get(payload)
    if hit is not None:
        return hit
    client = _get_openai_client()
    # Ù†Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø¬Ù‡Ø© Chat Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ openai>=1.0
    resp = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    text = resp.choices[0].message.content.strip()
    cache.set(payload, text)
    return text

# â€”â€” Ø§Ø³ØªØ¨Ø¯Ø§Ù„ use_container_width â€”â€” 
TABLE_WIDTH = 'stretch'  # Ø£Ùˆ 'content'

# =============== Ù‚ÙˆØ§Ù„Ø¨ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ù…Ø®ØªØµØ±Ø© (Ù…Ø¯Ù…Ø¬Ø©) ===============
BASE_TMPL = """# {title}
> Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {keyword}
> Ø§Ù„Ø£Ø³Ù„ÙˆØ¨: {tone_label}
> Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„ØªØ²Ù… Ø¨Ø¥Ø­Ø§Ù„Ø§Øª [^n] Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹.

## Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŸ
{tone_selection_line}

## Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±
{criteria_block}

## Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ù…Ø±Ø´Ø­Ø©
(Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥Ø­Ø§Ù„Ø§Øª [^n] Ø¹Ù†Ø¯ Ø°ÙƒØ± Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ø¯Ù†Ø§Ù‡.)

## Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©
{methodology_block}

## Ù…Ù„Ø§Ø­Ø¸Ø§Øª
{protip_hint}
"""

POLISH_TMPL = """Ø£Ø¹Ø¯ ØªØ­Ø±ÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ù„Ø§Ø³Ø©ØŒ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø´ÙˆØŒ ÙˆØ§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚. Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯Øª Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙƒØ§ØªØ¨ØŒ Ø¯Ù…Ø¬Ù‡Ø§ Ø¨Ù„Ø·Ù.
[Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„ÙƒØ§ØªØ¨]: {user_notes}

[Ø§Ù„Ù†Øµ]:
{article}
"""

FAQ_TMPL = """### Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
- Ù…Ø§ Ø£ÙØ¶Ù„ ÙˆÙ‚Øª Ù„Ù„Ø²ÙŠØ§Ø±Ø©ØŸ
- Ù‡Ù„ ØªØªÙˆÙØ± Ø¬Ù„Ø³Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©ØŸ
- Ù‡Ù„ Ø§Ù„Ù…ÙƒØ§Ù† Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ø§Ø¦Ù„Ø§ØªØŸ
"""

GENERAL_CRITERIA = """- Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
- Ø§Ù„Ø§ØªØ³Ø§Ù‚ Ø¹Ø¨Ø± Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª
- Ø§Ù„Ø³Ø¹Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù‚ÙŠÙ…Ø©
- Ø§Ù„Ù†Ø¸Ø§ÙØ© ÙˆØ³Ø±Ø¹Ø© Ø§Ù„Ø®Ø¯Ù…Ø©
"""

# =============== ÙˆØ¸Ø§Ø¦Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù…Ø§ÙƒÙ† (Ø¨Ø¯ÙŠÙ„ Ù…Ø¨Ø³Ø·) ===============
def _dedupe_places(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen, out = set(), []
    for r in rows:
        k = (r.get("phone") or "").strip() or (r.get("website") or "").strip() or slugify(r.get("name") or "")
        if k and k not in seen:
            seen.add(k); out.append(r)
    return out

def _score_place(row: Dict[str, Any], keyword: str) -> float:
    rating = float(row.get("rating") or 0.0)
    reviews = max(1, int(row.get("reviews_count") or 1))
    base = rating * math.log(reviews + 1.0)
    name = (row.get("name") or "").lower()
    kw = (keyword or "").lower().strip()
    boost_kw = 0.6 if kw and kw in name else 0.0
    boost_open = 0.3 if row.get("open_now") else 0.0
    return round(base + boost_kw + boost_open, 4)

def _extract_thursday_hours(row: Dict[str, Any]) -> str:
    hours_map = row.get("hours_map") or {}
    for key in ["Ø§Ù„Ø®Ù…ÙŠØ³", "Thursday", "Thu"]:
        if key in hours_map and hours_map[key]:
            return hours_map[key]
    ht = (row.get("hours_today") or "").strip()
    if "Ø§Ù„Ø®Ù…ÙŠØ³" in ht or "Thursday" in ht or "Thu" in ht:
        return ht.split(":", 1)[-1].strip()
    return "â€”"

def _build_references(rows: List[Dict[str, Any]]) -> List[str]:
    refs = []
    for r in rows:
        google_url = r.get("google_url") or r.get("gmaps_url") or "â€”"
        site = r.get("website") or "â€”"
        if site and site != "â€”":
            refs.append(f"{r.get('name','?')}: Google Maps {google_url} â€” Website {site}")
        else:
            refs.append(f"{r.get('name','?')}: Google Maps {google_url}")
    return refs

def search_places(keyword: str, city: str, min_reviews: int = 50) -> List[Dict[str, Any]]:
    """
    Ø¨Ø¯ÙŠÙ„ Ù…Ø¨Ø³Ù‘Ø·: ÙŠØ±Ø¬Ù‘Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙˆÙØ± Ù„Ø¯ÙŠÙƒ Ù…ÙˆÙÙ‘Ø± Ø®Ø§Ø±Ø¬ÙŠ.
    ÙŠÙ…ÙƒÙ†Ùƒ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ø¯Ù…Ø¬ Google Places API.
    """
    logger.info("places.request")
    sample = [
        {"name": "Burger Craft", "address": f"{city} - Ø§Ù„Ø­ÙŠ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠ", "phone": "0500000001",
         "website": "", "google_url": "https://maps.google.com/?q=Burger+Craft",
         "rating": 4.4, "reviews_count": 310, "price_band": "Ù…ØªÙˆØ³Ø·", "open_now": True,
         "hours_map": {"Thursday": "12:00â€“02:00"}},
        {"name": "Smash House", "address": f"{city} - Ø§Ù„ÙˆØ³Ø·Ù‰", "phone": "0500000002",
         "website": "https://smash.example", "google_url": "https://maps.google.com/?q=Smash+House",
         "rating": 4.2, "reviews_count": 190, "price_band": "Ø§Ù‚ØªØµØ§Ø¯ÙŠ", "open_now": False,
         "hours_map": {"Ø§Ù„Ø®Ù…ÙŠØ³": "13:00â€“01:00"}},
        {"name": "Flame & Bun", "address": f"{city} - Ø§Ù„ØºØ±Ø¨ÙŠØ©", "phone": "0500000003",
         "website": "", "google_url": "https://maps.google.com/?q=Flame+%26+Bun",
         "rating": 4.6, "reviews_count": 520, "price_band": "Ù…Ø±ØªÙØ¹", "open_now": True,
         "hours_map": {"Thu": "14:00â€“03:00"}},
    ]
    rows = [r for r in sample if int(r["reviews_count"]) >= min_reviews]
    logger.info("places.dedupe")
    return _dedupe_places(rows)

# =============== Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± ===============
def _criteria_normalize(raw) -> List[str]:
    """
    Ø¥ØµÙ„Ø§Ø­ ÙƒØ§Ù…Ù„ Ù„Ø®Ø·Ø£ Ø§Ù„Ø£Ù‚ÙˆØ§Ø³: Ù„Ø§ Ù†Ø³ØªØ¹Ù…Ù„ startswith(tuple).
    Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© ÙˆÙ†Ø±Ù…ÙŠ "undefined".
    """
    if raw is None:
        return []
    if isinstance(raw, str):
        s = raw.strip()
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª JSON-like: Ø§Ø¹ØªÙ…Ø¯ ÙØ­Øµ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø·
        if s and s[0] in "[{":
            try:
                raw = json.loads(s)
            except Exception:
                lines = [ln.strip(" -â€¢\t").strip() for ln in s.splitlines() if ln.strip()]
                return [ln for ln in lines if ln and ln.lower() != "undefined"]
        else:
            lines = [ln.strip(" -â€¢\t").strip() for ln in s.splitlines() if ln.strip()]
            return [ln for ln in lines if ln and ln.lower() != "undefined"]
    if isinstance(raw, dict):
        for k in ("criteria", "bullets", "items", "list"):
            if k in raw:
                raw = raw[k]; break
        else:
            vals = list(raw.values())
            raw = vals if all(isinstance(v, str) for v in vals) else list(raw.keys())
    if isinstance(raw, (list, tuple)):
        out = []
        for x in raw:
            if isinstance(x, str):
                t = x.strip().strip(",").strip('"').strip("'")
            elif isinstance(x, dict) and "text" in x:
                t = str(x["text"]).strip()
            else:
                t = str(x).strip()
            if t and t.lower() != "undefined":
                out.append(t)
        return out
    return [str(raw)]

def _format_criteria_md(items) -> str:
    items = _criteria_normalize(items)
    return "\n".join(f"- {c}" for c in items) or "- â€”"

# =============== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ===============
st.title("ğŸ½ï¸ Ù…ÙˆÙ„Ù‘Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù…â€”(Ù†Ø³Ø®Ø© Ù…Ø³ØªÙ‚Ù„Ø©)")

# â€”â€”â€” Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© â€”â€”â€”
st.sidebar.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
tone = st.sidebar.selectbox("Ù†ØºÙ…Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨",
    ["Ù†Ø§Ù‚Ø¯ ÙˆØ¯ÙˆØ¯","Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù…","Ø¯Ù„ÙŠÙ„ ØªØ­Ø±ÙŠØ±ÙŠ Ù…Ø­Ø§ÙŠØ¯"], index=0, key="tone_select")
primary_model = st.sidebar.selectbox("Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„", ["gpt-4.1","gpt-4o","gpt-4o-mini"], index=1, key="model_primary")
approx_len = st.sidebar.slider("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ (ÙƒÙ„Ù…Ø§Øª)", 600, 1800, 1100, step=100, key="approx_len_slider")
include_faq = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© FAQ", value=True, key="include_faq_chk")
include_methodology = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ø±ÙŠØ±", value=True, key="include_methodology_chk")

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ§  ÙƒØ§Ø´ LLM")
llm_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´", value=True, key="llm_cache_enabled_chk")
llm_cache_hours = st.sidebar.slider("Ù…Ø¯Ø© Ø§Ù„ÙƒØ§Ø´ (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="llm_cache_hours_slider")
st.session_state["llm_cache"].enabled = llm_cache_enabled
st.session_state["llm_cache"].ttl = llm_cache_hours * 3600

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ”— ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ø²Ø§Ù…ÙŠØ©")
mandatory_terms = _normalize_lines(st.sidebar.text_area(
    "Ø¹Ø¨Ø§Ø±Ø§Øª Ø¥Ù„Ø²Ø§Ù…ÙŠØ© (Ø³Ø·Ø± Ù„ÙƒÙ„ Ø¹Ù†ØµØ±)",
    value="Ù…Ø·Ø§Ø¹Ù… Ø¹Ø§Ø¦Ù„ÙŠØ©\nØ¬Ù„Ø³Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©\nÙ…ÙˆØ§Ù‚Ù Ø³ÙŠØ§Ø±Ø§Øª",
    height=100,
    key="mandatory_terms_ta",
))

# =============== ØªØ¨ÙˆÙŠØ¨Ø§Øª ===============
tab_places, tab_article, tab_qc = st.tabs(["ğŸ›°ï¸ Ø£Ù…Ø§ÙƒÙ†", "âœï¸ Ù…Ù‚Ø§Ù„", "ğŸ§ª Ø¬ÙˆØ¯Ø©"])

# ---------- ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† ----------
with tab_places:
    st.subheader("ğŸ›°ï¸ Ø¬Ù„Ø¨ ÙˆØªÙ†Ù‚ÙŠØ© Ø£Ù…Ø§ÙƒÙ† (Ø¨Ø¯ÙŠÙ„ Ù…Ø¨Ø³Ù‘Ø·)")
    col1, col2, col3, col4 = st.columns([2, 1.2, 1, 1])
    with col1:
        gp_keyword = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©", "Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶", key="gp_kw_in")
    with col2:
        gp_city = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", "Ø§Ù„Ø±ÙŠØ§Ø¶", key="gp_city_in")
    with col3:
        gp_min_reviews = st.number_input("Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª", min_value=0, max_value=5000, value=50, step=10, key="gp_min_reviews_num")
    with col4:
        btn_fetch = st.button("ğŸ“¥ Ø¬Ù„Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", key="btn_gp_fetch")

    if btn_fetch:
        rows = search_places(keyword=gp_keyword, city=gp_city, min_reviews=int(gp_min_reviews))
        for r in rows:
            r["score"] = _score_place(r, gp_keyword)
            r["hours_thursday"] = _extract_thursday_hours(r)
        rows.sort(key=lambda x: x["score"], reverse=True)
        if len(rows) < 6:
            st.warning("âš ï¸ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ù‚Ù„ Ù…Ù† 6 Ø¹Ù†Ø§ØµØ± â€” Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¶Ø¹ÙŠÙØ© Ù„Ù„Ù†Ø´Ø±.")
        st.dataframe(rows, width=TABLE_WIDTH)
        st.session_state["places_results"] = rows

    st.markdown("---")
    if st.button("âœ”ï¸ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©", key="btn_accept_places"):
        rows = st.session_state.get("places_results") or []
        if not rows:
            st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ø§Ø¹ØªÙ…Ø§Ø¯Ù‡Ø§.")
        else:
            snap = []
            for r in rows:
                snap.append({
                    "name": r.get("name"), "address": r.get("address"),
                    "phone": r.get("phone"), "website": r.get("website"),
                    "google_url": r.get("google_url") or r.get("gmaps_url"),
                    "rating": r.get("rating"), "reviews_count": r.get("reviews_count"),
                    "price_band": r.get("price_band"), "open_now": r.get("open_now"),
                    "hours_thursday": r.get("hours_thursday") or _extract_thursday_hours(r),
                })
            refs = _build_references(snap)
            st.session_state["places_snapshot"] = snap
            st.session_state["places_references"] = refs
            st.success("ØªÙ… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯.")

# ---------- ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„ ----------
with tab_article:
    st.subheader("âœï¸ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„")
    colA, colB = st.columns([2, 1])
    with colA:
        article_title = st.text_input("Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„", "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶", key="article_title_in")
        article_keyword = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", "Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶", key="article_kw_in_tab2")
        criteria_block = st.text_area("Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±", value=GENERAL_CRITERIA, height=120, key="criteria_ta")
        manual_notes = st.text_area("Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙŠØ¯ÙˆÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", height=120, key="notes_ta")
    with colB:
        include_jsonld = st.checkbox("ØªØ¶Ù…ÙŠÙ† JSON-LD", value=True, key="include_jsonld_chk")
        st.caption("ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØ© Ù…ÙØªØ§Ø­ OpenAI ÙÙŠ secrets Ø£Ùˆ env.")

    snap = st.session_state.get("places_snapshot") or []
    refs = st.session_state.get("places_references") or []
    st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©: **{len(snap)}**")
    if snap:
        names_preview = ", ".join([s["name"] for s in snap[:8]]) + ("â€¦" if len(snap) > 8 else "")
        st.info(f"Ø£Ø¨Ø±Ø² Ø§Ù„Ø£Ù…Ø§ÙƒÙ†: {names_preview}")

    if st.button("ğŸš€ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„", key="btn_generate_article"):
        if not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY."); st.stop()
        if not snap:
            st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚Ø§Ø¦Ù…Ø© Ø£Ù…Ø§ÙƒÙ† Ù…Ø¹ØªÙ…Ø¯Ø©."); st.stop()

        tone_selection_line = "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© ÙˆÙ…Ø±Ø§Ø¬Ø¹Ø§Øª Ø­ØªÙ‰ {last_updated}.".format(
            last_updated=datetime.now().strftime("%B %Y")
        )
        facts_lines = []
        for idx, s in enumerate(snap, start=1):
            facts_lines.append(
                f"- {s['name']} â€” Ø³Ø¹Ø±: {s.get('price_band','â€”')} â€” Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø®Ù…ÙŠØ³: {s.get('hours_thursday','â€”')} â€” Ù‡Ø§ØªÙ: {s.get('phone','â€”')} [^{idx}]"
            )
        facts_block = "\n".join(facts_lines)
        refs_block = "\n".join([f"[^{i+1}]: {r}" for i, r in enumerate(refs)])

        mandatory_hint = ""
        if mandatory_terms:
            mandatory_hint = "Ø£Ø¯Ø±Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ: " + ", ".join(f"â€œ{t}â€" for t in mandatory_terms) + "."

        base_prompt = BASE_TMPL.format(
            title=article_title,
            keyword=article_keyword,
            tone_label=st.session_state["tone_select"],
            tone_selection_line=tone_selection_line,
            criteria_block=_format_criteria_md(criteria_block),
            methodology_block=("Ø³ÙŠØ§Ø³Ø© ØªØ­Ø±ÙŠØ±ÙŠØ© Ù…Ø®ØªØµØ±Ø©." if include_methodology else "â€”"),
            protip_hint=mandatory_hint or "â€”",
        )

        system_tone = f"Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ØŒ Ø£Ø³Ù„ÙˆØ¨ {st.session_state['tone_select']}, Ø·ÙˆÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ {approx_len} ÙƒÙ„Ù…Ø©."
        messages = [
            {"role": "system", "content": system_tone},
            {"role": "user", "content":
                base_prompt
                + "\n\n---\n\n"
                + "## Ø­Ù‚Ø§Ø¦Ù‚ Ù…Ø¶ØºÙˆØ·Ø© Ø¹Ù† Ø§Ù„Ø£Ù…Ø§ÙƒÙ† (Ù„Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø¥Ø­Ø§Ù„Ø§Øª [^n]):\n"
                + facts_block
                + "\n\n## Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ (Ø¶Ø¹ [^n] Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯):\n"
                + refs_block
            },
        ]
        try:
            article_md = chat_complete_cached(messages, model=primary_model, max_tokens=2200, temperature=0.7)
        except Exception as e:
            st.error(f"ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„: {e}"); st.stop()

        # Ø·Ø¨Ù‚Ø© Polish + Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª
        if manual_notes.strip():
            try:
                article_md = chat_complete_cached(
                    [
                        {"role":"system","content":"Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ Ù…Ø­ØªØ±ÙØŒ ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ ÙˆØªÙ‚Ù„Ù„ Ø§Ù„Ø­Ø´Ùˆ."},
                        {"role":"user","content": POLISH_TMPL.format(article=article_md, user_notes=manual_notes)}
                    ],
                    model=primary_model, max_tokens=2200, temperature=0.6
                )
            except Exception as e:
                st.warning(f"ØªØ¹Ø°Ù‘Ø±Øª Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ­Ø±ÙŠØ±: {e}")

        # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© Ø¥Ø°Ø§ ØºØ§Ø¨Øª
        missing_terms = [t for t in mandatory_terms if t not in article_md]
        if missing_terms:
            try:
                article_md = chat_complete_cached(
                    [
                        {"role":"system","content":"Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ ØªÙØ¯Ø±Ø¬ Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø¯ÙˆÙ† Ø­Ø´Ùˆ."},
                        {"role":"user","content": f"Ø£Ø¯Ø±Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙ‚Ø· Ø­ÙŠØ« ØªÙ„Ø§Ø¦Ù… Ø§Ù„Ø³ÙŠØ§Ù‚: {', '.join(missing_terms)}.\n\nØ§Ù„Ù†Øµ:\n{article_md}"}
                    ],
                    model=primary_model, max_tokens=2000, temperature=0.4
                )
            except Exception:
                pass

        # SEO Title/Desc
        try:
            meta_out = chat_complete_cached(
                [
                    {"role":"system","content":"Ù…Ø®ØªØµ SEO Ø¹Ø±Ø¨ÙŠ."},
                    {"role":"user","content": f"ØµÙØº Ø¹Ù†ÙˆØ§Ù† SEO (â‰¤60) ÙˆÙˆØµÙ Ù…ÙŠØªØ§ (â‰¤155) Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ù†ÙˆØ§Ù† \"{article_title}\".\nTITLE: ...\nDESCRIPTION: ..."}
                ],
                model=primary_model, max_tokens=180, temperature=0.5
            )
        except Exception:
            meta_out = f"TITLE: {article_title}\nDESCRIPTION: Ø¯Ù„ÙŠÙ„ Ø¹Ù…Ù„ÙŠ Ø¹Ù† {article_keyword}."

        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„")
        st.markdown(article_md)
        st.session_state['last_article_md'] = article_md

        if include_faq:
            st.markdown(FAQ_TMPL)

        st.subheader("ğŸ” Meta (SEO)")
        st.code(meta_out, language="text")

        # ØªÙ†Ø²ÙŠÙ„Ø§Øª
        from docx import Document
        def to_docx(md_text: str) -> bytes:
            doc = Document()
            for line in md_text.splitlines():
                doc.add_paragraph(line)
            import io
            buf = io.BytesIO()
            doc.save(buf)
            return buf.getvalue()

        col1, col2 = st.columns(2)
        with col1:
            st.download_button('ğŸ’¾ ØªÙ†Ø²ÙŠÙ„ Markdown', data=article_md, file_name='article.md', mime='text/markdown', key="dl_md_btn")
        with col2:
            st.download_button('ğŸ“ ØªÙ†Ø²ÙŠÙ„ DOCX', data=to_docx(article_md),
                               file_name='article.docx',
                               mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                               key="dl_docx_btn")

# ---------- ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø¬ÙˆØ¯Ø© ----------
with tab_qc:
    st.subheader("ğŸ§ª ÙØ­Øµ Ø³Ø±ÙŠØ¹")
    qc_text = st.text_area("Ø§Ù„ØµÙ‚ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§", st.session_state.get("last_article_md",""), height=260, key="qc_text_ta")
    if st.button("ØªØ­Ù„ÙŠÙ„", key="btn_qc_analyze"):
        if not qc_text.strip():
            st.warning("Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            # ÙØ­Øµ Ù…Ø¨Ø³Ù‘Ø·: ÙƒØ«Ø§ÙØ© Ø§Ù„Ø­Ø´Ùˆ + ØªÙ†ÙˆØ¹ Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ø¬Ù…Ù„
            filler = ["Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±","Ø­Ù‚Ù‹Ø§","Ù„Ù„ØºØ§ÙŠØ©","Ù†ÙˆØ¹Ù‹Ø§ Ù…Ø§","ØªÙ…Ø§Ù…Ù‹Ø§","Ø¬Ø¯Ù‹Ø§ Ø¬Ø¯Ù‹Ø§"]
            words = qc_text.split()
            fluff_hits = sum(qc_text.count(f) for f in filler)
            avg_len = sum(len(s.split()) for s in qc_text.split(".")) / max(1, len(qc_text.split(".")))
            res = {
                "length_words": len(words),
                "fluff_hits": fluff_hits,
                "avg_sentence_len": round(avg_len, 2),
                "hint": "Ø®ÙÙ‘Ù Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ§Ø¯Ù…Ø¬ ØªÙØ§ØµÙŠÙ„ Ø­Ø³ÙŠØ© Ø£ÙƒØ«Ø±."
            }
            st.json(res)
            st.success("Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„.")
