# app.py
# -*- coding: utf-8 -*-

import os, sys, uuid, json, time, hashlib, math
from typing import List, Dict, Any, Optional, Tuple

ROOT = os.path.dirname(os.path.abspath(__file__))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

import streamlit as st
import pandas as pd

# === Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¯Ø§Ø®Ù„ÙŠØ© ===
from utils.logging_setup import init_logging, get_logger, set_correlation_id, log_exception
from utils.content_fetch import fetch_and_extract, configure_http_cache
from utils.openai_client import chat_complete_cached
from utils.exporters import to_docx, to_json
from utils.competitor_analysis import analyze_competitors, extract_gap
from utils.quality_checks import quality_report, build_jsonld
from utils.google_places import fetch_places_for_topic
from utils.keywords import related_keywords
from utils.human_check import human_likeness_report

# ========= ØªÙ‡ÙŠØ¦Ø© Ø¹Ø§Ù…Ø© =========
os.makedirs("data", exist_ok=True)
init_logging(app_name="restoguide", level=os.getenv("LOG_LEVEL", "INFO"))
logger = get_logger("app")
set_correlation_id(str(uuid.uuid4())[:8])

# ========= Helpers =========
def parse_urls_block(block: str) -> List[str]:
    urls = []
    for line in (block or "").splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith(("http://", "https://")):
            urls.append(s)
    return urls

def normalize_json_or_text(s: str) -> Tuple[Optional[Any], str]:
    s = (s or "").strip()
    if not s:
        return None, ""
    try:
        if s.startswith(("[", "{")):
            return json.loads(s), ""
    except Exception:
        pass
    return None, s

def hash_messages(messages: List[Dict[str, Any]], model: str) -> str:
    raw = json.dumps({"m": messages, "model": model}, ensure_ascii=False, sort_keys=True)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()

class LLMCacher:
    def __init__(self, ttl_hours: int = 24):
        self.ttl = ttl_hours * 3600
        self.mem: Dict[str, Tuple[float, Any]] = {}

    def get(self, key: str):
        row = self.mem.get(key)
        if not row:
            return None
        ts, val = row
        if time.time() - ts > self.ttl:
            self.mem.pop(key, None)
            return None
        return val

    def set(self, key: str, value: Any):
        self.mem[key] = (time.time(), value)

# ========= ÙˆØ§Ø¬Ù‡Ø© =========
st.set_page_config(page_title="RestoGuide", page_icon="ğŸ½ï¸", layout="wide")
st.title("ğŸ½ï¸ Ù…ÙÙˆÙ„Ù‘ÙØ¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (RestoGuide) â€” Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©")

st.markdown(
    "- ÙŠØ¬Ù„Ø¨ Ù…ØµØ§Ø¯Ø± Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªÙØ¯Ø®Ù„Ù‡Ø§.\n"
    "- **Ø§Ø®ØªÙŠØ§Ø±ÙŠÙ‹Ø§** ÙŠØ¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø·Ø§Ø¹Ù… Ù…Ù† **Ø®Ø±Ø§Ø¦Ø· Google** ÙˆÙŠØ¶Ù…Ù‘Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n"
    "- ÙŠØ³ØªØ®Ø±Ø¬ **Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©**ØŒ ÙˆÙŠÙ‚ØªØ±Ø­ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© ÙˆØµÙˆØ±.\n"
    "- ÙŠÙØ¬Ø±ÙŠ **ØªØ­Ù‚Ù‚Ù‹Ø§ ØªÙ‚Ø±ÙŠØ¨ÙŠÙ‹Ø§ Ù…Ù† Ø¨Ø´Ø±ÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰** Ù…Ø¹ Ù†Ù‚Ø§Ø·/ØªØ­Ù„ÙŠÙ„.\n"
    "- ÙØ­ÙˆØµØ§Øª **Ø¬ÙˆØ¯Ø©/SEO** (H1/H2ØŒ Ø·ÙˆÙ„ØŒ ÙƒØ«Ø§ÙØ©ØŒ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŒ ØªÙƒØ±Ø§Ø±ØŒ Ø±ÙˆØ§Ø¨Ø·â€¦).\n"
    "- **ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ†** ÙˆÙØ¬ÙˆØ§Øª Ù…Ø­ØªÙˆÙ‰.\n"
    "- **ØªØµØ¯ÙŠØ±** DOCX ÙˆJSONØŒ ÙˆØªØ¶Ù…ÙŠÙ† **Schema.org JSON-LD**."
)

# Sidebar
st.sidebar.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")

# ÙƒØ§Ø´ HTTP
st.sidebar.subheader("ğŸ›°ï¸ ÙƒØ§Ø´ HTTP")
http_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ HTTP", value=True, key="opt_http_cache_enabled")
http_cache_hours   = st.sidebar.slider("Ù…Ø¯Ø© Ø§Ù„ÙƒØ§Ø´ (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="opt_http_cache_hours")
configure_http_cache(ttl_hours=int(http_cache_hours), enabled=bool(http_cache_enabled))

st.sidebar.markdown("---")

# ÙƒØ§Ø´ LLM
st.sidebar.subheader("ğŸ§  ÙƒØ§Ø´ LLM")
llm_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ LLM", value=True, key="opt_llm_cache_enabled")
llm_cache_hours   = st.sidebar.slider("TTL LLM Cache (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="opt_llm_cache_hours")

if "llm_cacher" not in st.session_state:
    st.session_state["llm_cacher"] = LLMCacher(ttl_hours=int(llm_cache_hours))
else:
    st.session_state["llm_cacher"].ttl = int(llm_cache_hours) * 3600

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª LLM
st.sidebar.subheader("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡")
model_name = st.sidebar.selectbox(
    "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬",
    ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"],
    index=0,
    key="opt_model_name",
)
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.2, 0.1, key="opt_temperature")
max_tokens  = st.sidebar.slider("Max Tokens", 256, 8192, 2048, 64, key="opt_max_tokens")

st.sidebar.markdown("---")
st.sidebar.caption("Ø£Ø¶Ù `OPENAI_API_KEY` Ùˆ (Ø§Ø®ØªÙŠØ§Ø±ÙŠÙ‹Ø§) `GOOGLE_MAPS_API_KEY` ÙÙŠ Secrets.")

# Main inputs
st.subheader("ğŸ§¾ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„")
c1, c2, c3 = st.columns([2, 1.2, 1])
with c1:
    topic  = st.text_input("Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹/Ø§Ù„ÙØ¦Ø©", key="inp_topic", placeholder="Ù…Ø«Ø§Ù„: Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶")
with c2:
    area   = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©/Ø§Ù„Ù…Ù†Ø·Ù‚Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", key="inp_area", placeholder="Ø§Ù„Ø±ÙŠØ§Ø¶")
with c3:
    target_kw = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© (SEO)", key="inp_target_kw", placeholder="Ø¨Ø±Ø¬Ø± Ø§Ù„Ø±ÙŠØ§Ø¶ Ø§Ù„Ø£ÙØ¶Ù„")

tone = st.selectbox("Ù†Ø¨Ø±Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©", ["Ø§Ø­ØªØ±Ø§ÙÙŠ", "ÙˆØ¯ÙˆØ¯", "Ù…ØªØ­Ù…Ø³", "Ù…ÙˆØ¶ÙˆØ¹ÙŠ"], index=1, key="inp_tone")
lang = st.selectbox("Ø§Ù„Ù„ØºØ©", ["Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "English"], index=0, key="inp_lang")
desired_words = st.slider("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù (ÙƒÙ„Ù…Ø§Øª)", 400, 3000, 1200, 50, key="inp_words")

st.markdown("### ğŸ”— Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ø±ÙˆØ§Ø¨Ø·ØŒ Ø³Ø·Ø± Ù„ÙƒÙ„ Ø±Ø§Ø¨Ø·)")
urls_block = st.text_area("Ø£Ù„ØµÙ‚ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù‡Ù†Ø§:", key="inp_urls", height=140, placeholder="https://example.com/..")
urls = parse_urls_block(urls_block)

st.markdown("---")

# ====== Tabs ======
tab_sources, tab_places, tab_outline, tab_draft, tab_quality, tab_export = st.tabs(
    ["ğŸ“¥ Ø§Ù„Ù…ØµØ§Ø¯Ø±", "ğŸ“ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (Ø®Ø±Ø§Ø¦Ø· Ù‚ÙˆÙ‚Ù„)", "ğŸ—‚ï¸ Ø§Ù„Ù…Ø®Ø·Ø·", "âœï¸ Ø§Ù„Ù…Ø³ÙˆØ¯Ø©", "ğŸ§ª Ø§Ù„Ø¬ÙˆØ¯Ø©/SEO", "ğŸ“¤ Ø§Ù„ØªØµØ¯ÙŠØ±"]
)

# ---------- ğŸ“¥ Ø§Ù„Ù…ØµØ§Ø¯Ø± ----------
with tab_sources:
    st.subheader("Ø¬Ù„Ø¨/Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·")
    if st.button("Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·", key="btn_fetch_sources"):
        try:
            docs = fetch_and_extract(urls)
            st.session_state["docs"] = docs
            st.success(f"ØªÙ… Ø¬Ù„Ø¨ {len(docs)} Ù…ØµØ¯Ø±Ù‹Ø§.")
            for d in docs:
                with st.expander(d.get("url", "Ù…ØµØ¯Ø±"), expanded=False):
                    st.write(d.get("title") or "")
                    st.write(d.get("text")[:1500] + ("..." if len(d.get("text","")) > 1500 else ""))
        except Exception as e:
            log_exception(logger, e)
            st.error(f"ÙØ´Ù„ Ø§Ù„Ø¬Ù„Ø¨: {e}")

    cka, ckb, ckc = st.columns([1, 1, 1])
    with cka:
        if st.button("Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©", key="btn_related_keywords"):
            src_texts = [d.get("text","") for d in st.session_state.get("docs", [])]
            kws = related_keywords(topic=topic, target_kw=target_kw, texts=src_texts, model=model_name,
                                   temperature=temperature, max_tokens=max_tokens,
                                   llm_cacher=st.session_state["llm_cacher"] if llm_cache_enabled else None)
            st.session_state["related_kws"] = kws
            st.success("ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬/Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©.")
            st.dataframe(pd.DataFrame({"keyword": kws}), use_container_width=True)
    with ckb:
        if st.button("ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ†", key="btn_competitors"):
            docs = st.session_state.get("docs", [])
            comp = analyze_competitors(docs)
            st.session_state["competitors"] = comp
            st.success("ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†.")
            st.json(comp)
    with ckc:
        if st.button("ÙØ¬ÙˆØ§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰", key="btn_gap"):
            docs = st.session_state.get("docs", [])
            comp = st.session_state.get("competitors", {})
            gaps = extract_gap(topic=topic, docs=docs, competitor_summary=comp)
            st.session_state["gaps"] = gaps
            st.success("ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙØ¬ÙˆØ§Øª.")
            st.json(gaps)

# ---------- ğŸ“ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… ----------
with tab_places:
    st.subheader("Ø¬Ù„Ø¨ Ù…Ø·Ø§Ø¹Ù… Ù…Ù† Ø®Ø±Ø§Ø¦Ø· Google (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
    enable_places = st.checkbox("ØªÙØ¹ÙŠÙ„ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…", value=False, key="opt_enable_places")
    qcol1, qcol2 = st.columns([2,1])
    with qcol1:
        places_query = st.text_input("Ø¨Ø­Ø« (Ù…Ø«Ø§Ù„: Burger restaurants)", value="Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø±", key="inp_places_query")
    with qcol2:
        places_limit = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", 5, 50, 15, 1, key="inp_places_limit")
    if st.button("Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…", key="btn_fetch_places") and enable_places:
        try:
            places = fetch_places_for_topic(query=places_query, area=area, limit=places_limit)
            st.session_state["places"] = places
            st.success(f"ØªÙ… Ø¬Ù„Ø¨ {len(places)} Ù…Ø·Ø¹Ù…Ù‹Ø§.")
            if places:
                df = pd.DataFrame(places)
                st.dataframe(df, use_container_width=True)
        except Exception as e:
            log_exception(logger, e)
            st.error(f"ØªØ¹Ø°Ù‘ÙØ± Ø§Ù„Ø¬Ù„Ø¨ Ù…Ù† Places: {e}")

# ---------- ğŸ—‚ï¸ Ø§Ù„Ù…Ø®Ø·Ø· ----------
with tab_outline:
    st.subheader("ØªÙˆÙ„ÙŠØ¯ Ù…Ø®Ø·Ø· Ø§Ù„Ù…Ù‚Ø§Ù„")
    include_faq = st.checkbox("ØªØ¶Ù…ÙŠÙ† Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø©", value=True, key="opt_include_faq")
    include_schema = st.checkbox("ØªØ¶Ù…ÙŠÙ† Ø³ÙƒÙŠÙ…Ø§ JSON-LD", value=True, key="opt_include_schema")
    prompt_outline = f"""
Ø§ÙƒØªØ¨ Ù…Ø®Ø·Ø·Ù‹Ø§ Ø´Ø§Ù…Ù„Ù‹Ø§ Ù„Ù…Ù‚Ø§Ù„ Ø¹Ù†: "{topic}" ÙÙŠ Ù…Ù†Ø·Ù‚Ø© "{area}" Ø¨Ø§Ù„Ù„ØºØ© {lang}.
- Ù†Ø¨Ø±Ø©: {tone}. Ø·ÙˆÙ„ Ù…Ø³ØªÙ‡Ø¯Ù: {desired_words} ÙƒÙ„Ù…Ø©.
- Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {target_kw}
- Ø¥Ù† ÙˆÙØ¬Ø¯Øª ÙØ¬ÙˆØ§Øª Ù…Ø­ØªÙˆÙ‰ØŒ Ø¹Ø§Ù„Ø¬Ù‡Ø§.
- Ø¥Ù† ÙˆÙØ¬Ø¯Øª Ù‚Ø§Ø¦Ù…Ø© Ù…Ø·Ø§Ø¹Ù… (Ø§Ø³Ù…/ØªÙ‚ÙŠÙŠÙ…/Ø³Ø¹Ø±/Ø±Ø§Ø¨Ø· Ø®Ø±Ø§Ø¦Ø·)ØŒ Ø§Ù‚ØªØ±Ø­ Ø£Ù‚Ø³Ø§Ù…Ù‹Ø§ ØªÙ‚Ø§Ø±Ù† ÙˆØªØ´Ø±Ø­.
- Ø¹Ù†Ø§ÙˆÙŠÙ† H2/H3 ÙˆØ§Ø¶Ø­Ø© ÙˆØºÙ†ÙŠØ© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø­Ø´Ùˆ.
- Ø®Ø§ØªÙ…Ø© + CTA.
    """.strip()

    if st.button("ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø®Ø·Ø·", key="btn_gen_outline"):
        try:
            messages = [
                {"role":"system","content":"Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ­Ø±ÙŠØ± SEO ÙˆØµÙŠØ§ØºØ© Ù…Ø®Ø·Ø·Ø§Øª Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ø±Ø¨ÙŠØ© Ù…ØªÙŠÙ†Ø©."},
                {"role":"user","content": prompt_outline}
            ]
            cache_key = hash_messages(messages, model_name)
            if llm_cache_enabled and (cached := st.session_state["llm_cacher"].get(cache_key)):
                outline = cached
            else:
                outline = chat_complete_cached(messages, model=model_name, temperature=temperature, max_tokens=max_tokens)
                if llm_cache_enabled:
                    st.session_state["llm_cacher"].set(cache_key, outline)
            st.session_state["outline"] = outline
            st.success("ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø®Ø·Ø·.")
            st.text_area("Ø§Ù„Ù…Ø®Ø·Ø·", value=outline, key="out_outline", height=350)
        except Exception as e:
            log_exception(logger, e)
            st.error(f"ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø®Ø·Ø·: {e}")

# ---------- âœï¸ Ø§Ù„Ù…Ø³ÙˆØ¯Ø© ----------
with tab_draft:
    st.subheader("ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø³ÙˆØ¯Ø©")
    images_prompts = st.checkbox("Ø§Ù‚ØªØ±Ø§Ø­ Ø£ÙÙƒØ§Ø± ØµÙˆØ±/ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¨Ø¯ÙŠÙ„Ø©", value=True, key="opt_img_prompts")
    multi_titles   = st.checkbox("Ø§Ù‚ØªØ±Ø§Ø­ 5 Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø¯ÙŠÙ„Ø© Ø¬Ø°Ø§Ø¨Ø©", value=True, key="opt_multi_titles")
    include_places = st.checkbox("Ø¥Ø¯Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ Ù…Ø®ØªØµØ± Ù„Ù„Ù…Ø·Ø§Ø¹Ù… (Ø¥Ù† ÙˆÙØ¬Ø¯)", value=True, key="opt_include_places")
    include_kws    = st.checkbox("Ø¥Ø¨Ø±Ø§Ø² Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ", value=True, key="opt_include_kws")

    if st.button("ØªÙˆÙ„ÙŠØ¯ Ù…Ø³ÙˆØ¯Ø© Ø§Ù„Ù…Ù‚Ø§Ù„", key="btn_gen_draft"):
        try:
            docs_text = "\n\n".join([d.get("text","")[:5000] for d in st.session_state.get("docs", [])])
            places = st.session_state.get("places", [])
            places_block = json.dumps(places, ensure_ascii=False) if places and include_places else ""
            kws = st.session_state.get("related_kws", []) if include_kws else []

            outline = st.session_state.get("outline", "")
            prompt = f"""
Ø§ÙƒØªØ¨ Ù…Ø³ÙˆØ¯Ø© Ù…Ù‚Ø§Ù„ Ù…ÙƒØªÙ…Ù„Ø© Ø¹Ù† "{topic}" ÙÙŠ "{area}" Ø¨Ø§Ù„Ù„ØºØ© {lang} Ø¨Ù†Ø¨Ø±Ø© {tone}ØŒ Ø¨Ø·ÙˆÙ„ ÙŠÙ‚Ø§Ø±Ø¨ {desired_words} ÙƒÙ„Ù…Ø©.
Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {target_kw}
Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø¢ØªÙŠ Ø¥Ù† ÙˆÙØ¬Ø¯:
{outline}

Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªØµØ±Ø© (Ø§Ø³ØªÙ„Ù‡Ù… Ù…Ù†Ù‡Ø§ Ø¯ÙˆÙ† Ù†Ø³Ø®):
{docs_text[:4000]}

Ù‚Ø§Ø¦Ù…Ø© Ù…Ø·Ø§Ø¹Ù… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù‚Ø¯ ØªÙƒÙˆÙ† ÙØ§Ø±ØºØ© JSON):
{places_block}

Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ):
{kws}

Ù…ØªØ·Ù„Ø¨Ø§Øª:
- Ù…Ù‚Ø¯Ù…Ø© Ø¬Ø°Ø§Ø¨Ø©ØŒ Ø«Ù… Ø£Ù‚Ø³Ø§Ù… H2/H3ØŒ Ù†Ù‚Ø§Ø· Ù…Ø¹Ø¯Ù‘Ø¯Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©.
- Ù„Ø§ ØªÙ†Ø³Ø® Ø­Ø±ÙÙŠÙ‹Ø§ Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±ØŒ Ø¨Ù„ Ø£Ø¹Ø¯ Ø§Ù„ØµÙŠØ§ØºØ© ÙˆØ±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ…Ø©.
- Ø£Ø¯Ø±Ø¬ Ø¥Ù† Ø£Ù…ÙƒÙ† ÙÙ‚Ø±Ø© "ÙƒÙŠÙ Ø§Ø®ØªØ±Ù†Ø§/Ù‚ÙŠÙ‘Ù…Ù†Ø§" ÙˆÙ…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±.
- Ø£Ø®ØªÙ… Ø¨Ø®Ù„Ø§ØµØ© ÙˆÙ†Ø¯Ø§Ø¡ CTA.
- Ø§Ù„Ù„ØºØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ø®Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø­Ø´Ùˆ ÙˆØ§Ù„ØªÙƒØ±Ø§Ø±.
- Ø£Ø¶Ù (Ø¥Ù† Ø£Ù…ÙƒÙ†) Ù‚Ø³Ù… FAQ Ù…ÙˆØ¬Ø².
            """.strip()

            messages = [
                {"role":"system","content":"Ø£Ù†Øª ÙƒØ§ØªØ¨ Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ Ø®Ø¨ÙŠØ± SEOØŒ ÙŠÙƒØªØ¨ Ù†ØµÙ‹Ø§ Ø·Ø¨ÙŠØ¹ÙŠÙ‹Ø§ ÙˆØ¨Ø´Ø±ÙŠÙ‹Ø§."},
                {"role":"user","content": prompt}
            ]
            cache_key = hash_messages(messages, model_name)
            if llm_cache_enabled and (cached := st.session_state["llm_cacher"].get(cache_key)):
                draft = cached
            else:
                draft = chat_complete_cached(messages, model=model_name, temperature=temperature, max_tokens= max(1024, max_tokens))
                if llm_cache_enabled:
                    st.session_state["llm_cacher"].set(cache_key, draft)

            # Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØµÙˆØ± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            extras = {}
            if multi_titles:
                messages2 = [
                    {"role":"system","content":"Copywriter Ø¹Ø±Ø¨ÙŠ Ù…Ø­ØªØ±Ù Ø¹Ù†Ø§ÙˆÙŠÙ† SEO Ø¬Ø°Ø§Ø¨Ø© Ø¨Ù„Ø§ Ù†Ù‚Ø± Ø·Ø¹Ù…ÙŠ."},
                    {"role":"user","content": f"Ø§Ù‚ØªØ±Ø­ 5 Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø¯ÙŠÙ„Ø© Ù„Ù…Ù‚Ø§Ù„ Ø¹Ù†: {topic}ØŒ Ù†Ø¨Ø±Ø© {tone}, Ù„ØºØ© {lang}. Ø­Ø¯ Ø£Ù‚ØµÙ‰ 60 Ø­Ø±ÙÙ‹Ø§."}
                ]
                extras["titles"] = chat_complete_cached(messages2, model=model_name, temperature=0.5, max_tokens=256)

            if images_prompts:
                messages3 = [
                    {"role":"system","content":"Ù…ØµÙ…Ù… Ù…Ø­ØªÙˆÙ‰ ÙŠÙ‚ØªØ±Ø­ Ø£ÙÙƒØ§Ø± ØµÙˆØ± Alt Ù…ÙÙŠØ¯Ø© Ù„Ù„Ù‚Ø§Ø±Ø¦."},
                    {"role":"user","content": f"Ø£Ø¹Ø·Ù†ÙŠ 5 Ø£ÙÙƒØ§Ø± ØµÙˆØ± Ù…ÙˆØ¬Ø²Ø© Ù…Ø¹ Ù†Øµ Alt Ù…Ù†Ø§Ø³Ø¨ Ù„Ù…Ù‚Ø§Ù„ Ø¹Ù† {topic} ÙÙŠ {area}."}
                ]
                extras["images"] = chat_complete_cached(messages3, model=model_name, temperature=0.6, max_tokens=256)

            st.session_state["draft"] = draft
            st.session_state["extras"] = extras
            st.success("ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø³ÙˆØ¯Ø©.")
            st.text_area("Ø§Ù„Ù…Ø³ÙˆØ¯Ø©", value=draft, key="out_draft", height=420)

            if extras.get("titles"):
                st.markdown("**Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ù‚ØªØ±Ø­Ø©:**")
                st.code(extras["titles"])

            if extras.get("images"):
                st.markdown("**Ø£ÙÙƒØ§Ø± ØµÙˆØ±/Alt:**")
                st.code(extras["images"])

        except Exception as e:
            log_exception(logger, e)
            st.error(f"ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø³ÙˆØ¯Ø©: {e}")

# ---------- ğŸ§ª Ø§Ù„Ø¬ÙˆØ¯Ø©/SEO ----------
with tab_quality:
    st.subheader("ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©")
    draft_txt = st.session_state.get("draft", "")
    docs = st.session_state.get("docs", [])
    places = st.session_state.get("places", [])
    kws = st.session_state.get("related_kws", [])

    if st.button("ØªØ­Ù‚Ù‚ Ø¨Ø´Ø±ÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰", key="btn_human_check"):
        report = human_likeness_report(draft_txt, sources_text="\n\n".join(d.get("text","") for d in docs))
        st.session_state["human_report"] = report
        st.success("ØªÙ… ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¨Ø´Ø±ÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰.")
        st.json(report)

    if st.button("ÙØ­Øµ Ø¬ÙˆØ¯Ø©/SEO", key="btn_quality"):
        qr = quality_report(
            draft_txt=draft_txt,
            target_kw=target_kw,
            topic=topic,
            related_kws=kws,
            places=places,
            desired_words=desired_words
        )
        st.session_state["quality"] = qr
        st.success("ØªÙ… ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø©.")
        st.json(qr)

    if st.checkbox("Ø¹Ø±Ø¶ JSON-LD Schema", value=True, key="opt_show_schema"):
        jsonld = build_jsonld(topic=topic, area=area, draft=draft_txt, places=places, language=lang)
        st.code(json.dumps(jsonld, ensure_ascii=False, indent=2))

# ---------- ğŸ“¤ Ø§Ù„ØªØµØ¯ÙŠØ± ----------
with tab_export:
    st.subheader("ØªØµØ¯ÙŠØ±")
    filename = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯", value="article", key="inp_filename")

    cdoc, cjson = st.columns(2)
    with cdoc:
        if st.button("ØªØµØ¯ÙŠØ± DOCX", key="btn_export_docx"):
            try:
                path = to_docx(
                    filename=f"data/{filename}.docx",
                    title=topic,
                    draft=st.session_state.get("draft",""),
                    extras=st.session_state.get("extras", {}),
                )
                st.success(f"ØªÙ… Ø§Ù„Ø­ÙØ¸: {path}")
                st.download_button("ØªØ­Ù…ÙŠÙ„ DOCX", data=open(path, "rb").read(), file_name=os.path.basename(path), key="dl_docx")
            except Exception as e:
                log_exception(logger, e)
                st.error(f"ÙØ´Ù„ Ø§Ù„ØªØµØ¯ÙŠØ± DOCX: {e}")

    with cjson:
        if st.button("ØªØµØ¯ÙŠØ± JSON", key="btn_export_json"):
            try:
                payload = {
                    "topic": topic,
                    "area": area,
                    "target_kw": target_kw,
                    "outline": st.session_state.get("outline",""),
                    "draft": st.session_state.get("draft",""),
                    "extras": st.session_state.get("extras", {}),
                    "places": st.session_state.get("places", []),
                    "related_kws": st.session_state.get("related_kws", []),
                }
                path = to_json(f"data/{filename}.json", payload)
                st.success(f"ØªÙ… Ø§Ù„Ø­ÙØ¸: {path}")
                st.download_button("ØªØ­Ù…ÙŠÙ„ JSON", data=open(path, "rb").read(), file_name=os.path.basename(path), key="dl_json")
            except Exception as e:
                log_exception(logger, e)
                st.error(f"ÙØ´Ù„ Ø§Ù„ØªØµØ¯ÙŠØ± JSON: {e}")
