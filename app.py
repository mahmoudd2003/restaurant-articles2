# app.py
# -*- coding: utf-8 -*-

# =============================
#  ØªØ«Ø¨ÙŠØª Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
# =============================
import os, sys, uuid, json, time, hashlib
from typing import List, Dict, Any, Optional, Tuple

ROOT = os.path.dirname(os.path.abspath(__file__))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

# ===========
#  Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª
# ===========
import streamlit as st

# --------------- Ø¨Ø¯Ø§Ø¦Ù„ / Ù„ÙˆØ¬ÙŠÙ†Øº ---------------
try:
    from utils.logging_setup import (
        init_logging,
        get_logger,
        set_correlation_id,
        with_context,
        log_exception,
    )
except Exception:
    import logging

    _logger = logging.getLogger("restoguide")
    if not _logger.handlers:
        h = logging.StreamHandler()
        fmt = logging.Formatter("%(asctime)s | %(levelname)-7s | %(name)s | %(message)s")
        h.setFormatter(fmt)
        _logger.addHandler(h)
        _logger.setLevel(logging.INFO)

    def init_logging(app_name: str = "restoguide", level: str = "INFO"):
        _logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    def get_logger(name: str = "app"):
        return _logger

    def set_correlation_id(_cid: str):
        # no-op fallback
        pass

    def with_context(**kwargs):
        # decorator fallback
        def deco(fn):
            def wrapper(*a, **k):
                return fn(*a, **k)
            return wrapper
        return deco

    def log_exception(e: Exception):
        _logger.exception(e)

# --------------- Ø¨Ø¯Ø§Ø¦Ù„ / Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ ---------------
try:
    from utils.content_fetch import fetch_and_extract, configure_http_cache
except Exception:
    # Fallback Ø¨Ø³ÙŠØ· ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ requests + trafilatura/bs4 Ø¥Ù† ÙˆÙØ¬Ø¯Øª
    def configure_http_cache(ttl_hours: int = 24, enabled: bool = True):
        try:
            if enabled:
                import requests_cache
                requests_cache.install_cache(
                    cache_name="http_cache",
                    expire_after=ttl_hours * 3600,
                )
        except Exception:
            pass

    def fetch_and_extract(url: str) -> Dict[str, Any]:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù†Øµ Ø§Ù„ØµÙØ­Ø©. Fallback Ø¨Ø³ÙŠØ·."""
        try:
            import requests
            from bs4 import BeautifulSoup  # type: ignore
        except Exception:
            return {"url": url, "title": "", "text": ""}

        try:
            resp = requests.get(url, timeout=20)
            resp.raise_for_status()
            title = ""
            text = ""
            try:
                from trafilatura import extract  # type: ignore
                text = extract(resp.text) or ""
            except Exception:
                soup = BeautifulSoup(resp.text, "lxml")
                title_tag = soup.find("title")
                title = title_tag.get_text(strip=True) if title_tag else ""
                text = soup.get_text(" ", strip=True)
            return {"url": url, "title": title, "text": text}
        except Exception:
            return {"url": url, "title": "", "text": ""}

# --------------- Ø¨Ø¯Ø§Ø¦Ù„ / Ø¹Ù…ÙŠÙ„ OpenAI ---------------
try:
    from utils.openai_client import get_client, chat_complete_cached
except Exception:
    # Fallback ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ openai==1.x
    def get_client():
        from openai import OpenAI
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is not set")
        return OpenAI(api_key=api_key)

    def chat_complete_cached(messages: List[Dict[str, Any]], model: str = "gpt-4o-mini", **kwargs):
        client = get_client()
        return client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=kwargs.get("temperature", 0.2),
            max_tokens=kwargs.get("max_tokens"),
            top_p=kwargs.get("top_p", 1),
            presence_penalty=kwargs.get("presence_penalty"),
            frequency_penalty=kwargs.get("frequency_penalty"),
            seed=kwargs.get("seed"),
        )

# --------------- Ø¨Ø¯Ø§Ø¦Ù„ / Ø§Ù„ØªØµØ¯ÙŠØ± ---------------
try:
    from utils.exporters import to_docx, to_json
except Exception:
    def to_docx(article_text: str, filename: str) -> str:
        try:
            from docx import Document  # python-docx
        except Exception:
            # Ø¨Ø¯ÙŠÙ„ Ø¨Ø³ÙŠØ·: Ø£Ù†Ø´Ø¦ Ù…Ù„Ù txt Ø¨Ù†ÙØ³ Ø§Ù„Ø§Ø³Ù…
            path = os.path.join("data", filename.replace(".docx", ".txt"))
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, "w", encoding="utf-8") as f:
                f.write(article_text)
            return path

        doc = Document()
        for line in article_text.splitlines():
            doc.add_paragraph(line if line.strip() else "")
        os.makedirs("data", exist_ok=True)
        path = os.path.join("data", filename)
        doc.save(path)
        return path

    def to_json(data: Any, filename: str) -> str:
        os.makedirs("data", exist_ok=True)
        path = os.path.join("data", filename)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return path

# --------------- Ø¨Ø¯Ø§Ø¦Ù„ / ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ† + Ø¬ÙˆØ¯Ø© ---------------
try:
    from utils.competitor_analysis import analyze_competitors, extract_gap
except Exception:
    def analyze_competitors(texts: List[str]) -> Dict[str, Any]:
        return {"summary": "n/a", "keywords": [], "gaps": []}

    def extract_gap(analysis: Dict[str, Any]) -> List[str]:
        return analysis.get("gaps", [])

try:
    from utils.quality_checks import quality_report
except Exception:
    def quality_report(text: str) -> Dict[str, Any]:
        return {"readability": "n/a", "coverage": "n/a", "notes": []}

# ===========
#  ØªÙ‡ÙŠØ¦Ø© Ø¹Ø§Ù…
# ===========
os.makedirs("data", exist_ok=True)
init_logging(app_name="restoguide", level=os.getenv("LOG_LEVEL", "INFO"))
logger = get_logger("app")
set_correlation_id(str(uuid.uuid4())[:8])

# =========
#  Helpers
# =========
def parse_urls_block(block: str) -> List[str]:
    urls = []
    for line in (block or "").splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith(("http://", "https://")):
            urls.append(s)
    return urls

def normalize_json_or_text(s: str) -> Tuple[Optional[Any], str]:
    """
    ÙŠØ­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© s ÙƒÙ€ JSON Ø¥Ø°Ø§ Ø¨Ø¯Ø£ Ø¨Ù€ { Ø£Ùˆ [  (ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ù‚ÙˆØ§Ø³ Ù‡Ù†Ø§!)
    """
    s = (s or "").strip()
    if not s:
        return None, ""
    try:
        # Ù…Ù‡Ù…: tuple ÙˆÙ„ÙŠØ³ list Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„Ù€ SyntaxError
        if s.startswith(("[", "{")):
            return json.loads(s), ""
    except Exception:
        pass
    return None, s

def hash_messages(messages: List[Dict[str, Any]], model: str) -> str:
    raw = json.dumps({"m": messages, "model": model}, ensure_ascii=False, sort_keys=True)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()

# ---------------
#  ÙƒØ§Ø´ LLM Ø¨Ø³ÙŠØ·
# ---------------
class LLMCacher:
    def __init__(self, ttl_hours: int = 24):
        self.ttl = ttl_hours * 3600
        self.mem: Dict[str, Tuple[float, Any]] = {}

    def get(self, key: str):
        row = self.mem.get(key)
        if not row:
            return None
        ts, val = row
        if time.time() - ts > self.ttl:
            self.mem.pop(key, None)
            return None
        return val

    def set(self, key: str, value: Any):
        self.mem[key] = (time.time(), value)

# =========
#  ÙˆØ§Ø¬Ù‡Ø© UI
# =========
st.set_page_config(page_title="RestoGuide", page_icon="ğŸ½ï¸", layout="wide")
st.title("ğŸ½ï¸ Ù…ÙÙˆÙ„Ù‘Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (RestoGuide)")

st.markdown(
    "Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠÙÙˆÙ„Ù‘Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª/Ø£Ø¯Ù„Ø© Ù…Ø·Ø§Ø¹Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…ÙˆØ§Ù‚Ø¹ ÙˆØ±ÙˆØ§Ø¨Ø· ØªÙØ²ÙˆÙ‘Ø¯Ù‡Ø§ Ø£Ù†ØªØŒ "
    "Ù…Ø¹ Ø¯Ø¹Ù… ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ† ÙˆÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ø®ØªÙŠØ§Ø±ÙŠ."
)

# Sidebar â€” Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
st.sidebar.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")

# ÙƒØ§Ø´ HTTP
st.sidebar.subheader("ğŸ›°ï¸ ÙƒØ§Ø´ HTTP")
http_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ HTTP", value=True, key="http_cache_enabled")
http_cache_hours = st.sidebar.slider("Ù…Ø¯Ø© (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="http_cache_hours")
configure_http_cache(ttl_hours=int(http_cache_hours), enabled=bool(http_cache_enabled))

st.sidebar.markdown("---")

# ÙƒØ§Ø´ LLM
st.sidebar.subheader("ğŸ§  ÙƒØ§Ø´ LLM")
llm_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ LLM", value=True, key="llm_cache_enabled")
llm_cache_hours = st.sidebar.slider("Ù…Ø¯Ø© (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="llm_cache_hours")

if "llm_cacher" not in st.session_state:
    st.session_state["llm_cacher"] = LLMCacher(ttl_hours=int(llm_cache_hours))
else:
    # Ø­Ø¯Ù‘Ø« TTL Ø¥Ø°Ø§ ØªØºÙŠÙ‘Ø±
    st.session_state["llm_cacher"].ttl = int(llm_cache_hours) * 3600

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª LLM
st.sidebar.subheader("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡")
model_name = st.sidebar.selectbox(
    "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬",
    ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"],
    index=0,
    key="model_name",
)
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.2, 0.1, key="temperature")
max_tokens = st.sidebar.slider("Max Tokens", 256, 8192, 2048, 64, key="max_tokens")

st.sidebar.markdown("---")
st.sidebar.caption("ØªØ£ÙƒØ¯ Ù…Ù† ØªØ¹ÙŠÙŠÙ† `OPENAI_API_KEY` ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„.")

# Main inputs
st.subheader("ğŸ§¾ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„")
col1, col2 = st.columns([2, 1])

with col1:
    topic = st.text_input("Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹/Ø§Ù„ÙØ¦Ø© (Ù…Ø«Ø§Ù„: Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶)", key="topic")
    area = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©/Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", key="area")
    tone = st.selectbox(
        "Ø§Ù„Ù†Ø¨Ø±Ø© Ø§Ù„ÙƒØªØ§Ø¨ÙŠØ©",
        ["Ø§Ø­ØªØ±Ø§ÙÙŠØ©", "Ø­Ù…Ø§Ø³ÙŠØ©", "ÙˆØ¯Ù‘ÙŠØ©", "Ù…Ø®ØªØµØ±Ø©", "Ø³ÙŠØ§Ø­ÙŠØ©"],
        index=0,
        key="tone",
    )
    length = st.selectbox("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ", ["Ù‚ØµÙŠØ±", "Ù…ØªÙˆØ³Ø·", "Ø·ÙˆÙŠÙ„"], index=1, key="length")

with col2:
    do_comp = st.checkbox("ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†", value=False, key="do_comp")
    do_quality = st.checkbox("ØªØ´ØºÙŠÙ„ ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø©", value=True, key="do_quality")
    out_docx = st.checkbox("ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù DOCX", value=True, key="out_docx")
    out_json = st.checkbox("ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù JSON", value=True, key="out_json")

st.subheader("ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ù…ØµØ§Ø¯Ø± (ÙˆØ§Ø­Ø¯Ø© ÙÙŠ ÙƒÙ„ Ø³Ø·Ø±)")
urls_block = st.text_area("Ø£Ù„ØµÙ‚ Ø±ÙˆØ§Ø¨Ø· Ù…Ù‚Ø§Ù„Ø§Øª/Ù‚ÙˆØ§Ø¦Ù… Ù…Ø·Ø§Ø¹Ù…", height=120, key="urls_block")
urls = parse_urls_block(urls_block)

st.subheader("ğŸ“ Ù…Ù„Ø§Ø­Ø¸Ø§Øª/ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
notes = st.text_area("Ø§ÙƒØªØ¨ Ø£ÙŠ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© ØªØ±ØºØ¨ ØªØ¶Ù…ÙŠÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„", height=100, key="notes")

generate = st.button("ğŸš€ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¢Ù†", type="primary", use_container_width=False)

# ==========================
#       Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
# ==========================
def build_prompt(topic: str, area: str, tone: str, length: str, notes: str, sources: List[Dict[str, Any]]):
    sys_prompt = (
        "Ø£Ù†Øª Ù…Ø­Ø±Ù‘Ø± Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªØµ ÙÙŠ Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·Ø§Ø¹Ù…. Ø§ÙƒØªØ¨ Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ø¶Ø­Ø©ØŒ Ù…ÙØ­Ø§ÙŠØ¯Ø©ØŒ "
        "Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø£ÙØ¶Ù„ Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„Ø³ÙŠÙˆØŒ Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© ÙˆÙ†ÙÙ‚Ø§Ø· ÙˆÙ‚ÙˆØ§Ø¦Ù… Ø­ÙŠØ« ÙŠÙ„Ø²Ù…."
    )

    user_parts = []
    if topic:
        user_parts.append(f"Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: {topic}")
    if area:
        user_parts.append(f"Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ: {area}")
    user_parts.append(f"Ø§Ù„Ù†Ø¨Ø±Ø©: {tone}")
    user_parts.append(f"Ø§Ù„Ø·ÙˆÙ„: {length}")

    if notes.strip():
        user_parts.append(f"Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: {notes.strip()}")

    if sources:
        src_short = []
        for s in sources:
            title = (s.get("title") or "").strip()
            url = s.get("url") or ""
            txt = (s.get("text") or "")[:1200]
            src_short.append({"title": title, "url": url, "snippet": txt})
        user_parts.append("Ù…Ù„Ø®Øµ Ù„Ù„Ù…ØµØ§Ø¯Ø± (Ø¹ÙŠÙ†Ø©):\n" + json.dumps(src_short, ensure_ascii=False, indent=2))

    return [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": "\n\n".join(user_parts)},
    ]

def call_llm(messages: List[Dict[str, Any]], model: str) -> str:
    cache_key = hash_messages(messages, model)
    if llm_cache_enabled:
        cached = st.session_state["llm_cacher"].get(cache_key)
        if cached:
            return cached

    resp = chat_complete_cached(messages=messages, model=model, temperature=temperature, max_tokens=max_tokens)
    # Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ openai==1.x
    text = ""
    try:
        text = resp.choices[0].message.content  # type: ignore
    except Exception:
        text = str(resp)

    if llm_cache_enabled:
        st.session_state["llm_cacher"].set(cache_key, text)
    return text

def fetch_sources(urls: List[str]) -> List[Dict[str, Any]]:
    if not urls:
        return []
    logger.info("restoguide | %s | app | places.fetch.start", str(uuid.uuid4())[:8])
    out = []
    for i, u in enumerate(urls):
        logger.info("restoguide | - | places | places.request")
        data = fetch_and_extract(u)
        out.append(data)
    # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª Ø­Ø³Ø¨ URL
    dedup = {}
    for row in out:
        dedup[row.get("url")] = row
    logger.info("restoguide | - | places | places.dedupe")
    rows = list(dedup.values())
    logger.info("restoguide | - | app | places.fetch.done")
    return rows

def export_outputs(article_text: str, meta: Dict[str, Any]):
    downloads = []
    if out_docx:
        docx_name = f"article_{int(time.time())}.docx"
        path = to_docx(article_text, docx_name)
        downloads.append(("DOCX", path))
    if out_json:
        json_name = f"article_{int(time.time())}.json"
        path = to_json(meta, json_name)
        downloads.append(("JSON", path))
    return downloads

# ==========================
#         Ø§Ù„ØªÙ†ÙÙŠØ°
# ==========================
if generate:
    if not topic.strip():
        st.error("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙˆØ¶ÙˆØ¹/ÙØ¦Ø© Ù„Ù„Ù…Ù‚Ø§Ù„.")
        st.stop()

    # Ø¬Ù„Ø¨ Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ø¥Ù† ÙˆØ¬Ø¯Øª Ø±ÙˆØ§Ø¨Ø·)
    sources = fetch_sources(urls) if urls else []

    # Ø±Ø³Ø§Ø¦Ù„ LLM
    msgs = build_prompt(topic=topic, area=area, tone=tone, length=length, notes=notes, sources=sources)

    # Ø§ØªØµØ§Ù„ LLM
    try:
        article_text = call_llm(msgs, model=model_name)
        logger.info("restoguide | - | app | places.accepted")
    except Exception as e:
        log_exception(e)
        st.exception(e)
        st.stop()

    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙƒÙ€ JSON Ø£Ùˆ Ù†Øµ
    parsed, plain = normalize_json_or_text(article_text)
    final_text = ""
    meta: Dict[str, Any] = {
        "topic": topic,
        "area": area,
        "tone": tone,
        "length": length,
        "notes": notes,
        "sources_count": len(sources),
        "model": model_name,
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    }

    if isinstance(parsed, dict) and parsed.get("article"):
        final_text = str(parsed.get("article"))
        meta["structure"] = "json"
        meta["raw"] = parsed
    else:
        final_text = plain or article_text
        meta["structure"] = "text"

    # ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    if do_comp and sources:
        comp = analyze_competitors([s.get("text", "") for s in sources])
        meta["competitor_analysis"] = comp
        meta["content_gaps"] = extract_gap(comp)

    # ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    if do_quality:
        meta["quality_report"] = quality_report(final_text)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    st.subheader("âœï¸ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬")
    st.text_area("Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ", value=final_text, height=400, key="final_article", label_visibility="collapsed")

    # ØªÙ†Ø²ÙŠÙ„
    st.subheader("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„Ø§Øª")
    downloads = export_outputs(final_text, meta)
    if not downloads:
        st.info("Ù„Ù… ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ Ø£ÙŠ Ø®ÙŠØ§Ø± ØªÙ†Ø²ÙŠÙ„.")
    else:
        for kind, path in downloads:
            st.markdown(f"- **{kind}**: [ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù]({path})")

# ===============
#  ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¨Ø³ÙŠØ·Ø©
# ===============
st.markdown("---")
st.caption("Â© 2025 RestoGuide â€” ÙŠØ¹Ù…Ù„ Ø¨ÙˆØ§Ø³Ø·Ø© Streamlit Ùˆ OpenAI. ")
