# app.py
# -*- coding: utf-8 -*-

import os, sys, uuid, json, time, hashlib
from typing import List, Dict, Any, Optional, Tuple

ROOT = os.path.dirname(os.path.abspath(__file__))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

import streamlit as st

# === Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¯Ø§Ø®Ù„ÙŠØ© (Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ utils Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø§Ùƒ) ===
from utils.logging_setup import (
    init_logging, get_logger, set_correlation_id, log_exception
)
from utils.content_fetch import fetch_and_extract, configure_http_cache
from utils.openai_client import chat_complete_cached
from utils.exporters import to_docx, to_json
from utils.competitor_analysis import analyze_competitors, extract_gap
from utils.quality_checks import quality_report

# ===========
#  ØªÙ‡ÙŠØ¦Ø© Ø¹Ø§Ù…
# ===========
os.makedirs("data", exist_ok=True)
init_logging(app_name="restoguide", level=os.getenv("LOG_LEVEL", "INFO"))
logger = get_logger("app")
set_correlation_id(str(uuid.uuid4())[:8])

# =========
#  Helpers
# =========
def parse_urls_block(block: str) -> List[str]:
    urls = []
    for line in (block or "").splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith(("http://", "https://")):
            urls.append(s)
    return urls

def normalize_json_or_text(s: str) -> Tuple[Optional[Any], str]:
    """
    ÙŠØ­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© s ÙƒÙ€ JSON Ø¥Ø°Ø§ Ø¨Ø¯Ø£ Ø¨Ù€ { Ø£Ùˆ [
    (Ø£ÙØµÙ„Ø­Øª Ø§Ù„Ø£Ù‚ÙˆØ§Ø³: tuple Ø¯Ø§Ø®Ù„ startswith)
    """
    s = (s or "").strip()
    if not s:
        return None, ""
    try:
        if s.startswith(("[", "{")):
            return json.loads(s), ""
    except Exception:
        pass
    return None, s

def hash_messages(messages: List[Dict[str, Any]], model: str) -> str:
    raw = json.dumps({"m": messages, "model": model}, ensure_ascii=False, sort_keys=True)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()

class LLMCacher:
    def __init__(self, ttl_hours: int = 24):
        self.ttl = ttl_hours * 3600
        self.mem: Dict[str, Tuple[float, Any]] = {}

    def get(self, key: str):
        row = self.mem.get(key)
        if not row:
            return None
        ts, val = row
        if time.time() - ts > self.ttl:
            self.mem.pop(key, None)
            return None
        return val

    def set(self, key: str, value: Any):
        self.mem[key] = (time.time(), value)

# =========
#  ÙˆØ§Ø¬Ù‡Ø© UI
# =========
st.set_page_config(page_title="RestoGuide", page_icon="ğŸ½ï¸", layout="wide")
st.title("ğŸ½ï¸ Ù…ÙÙˆÙ„Ù‘Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (RestoGuide)")

st.markdown(
    "Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠÙÙˆÙ„Ù‘Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª/Ø£Ø¯Ù„Ø© Ù…Ø·Ø§Ø¹Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· ØªÙØ²ÙˆÙ‘Ø¯Ù‡Ø§ØŒ "
    "Ù…Ø¹ Ø¯Ø¹Ù… ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ† ÙˆÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ø®ØªÙŠØ§Ø±ÙŠ."
)

# Sidebar â€” Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
st.sidebar.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")

# ÙƒØ§Ø´ HTTP
st.sidebar.subheader("ğŸ›°ï¸ ÙƒØ§Ø´ HTTP")
http_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ HTTP", value=True, key="opt_http_cache_enabled")
http_cache_hours   = st.sidebar.slider("Ù…Ø¯Ø© (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="opt_http_cache_hours")
configure_http_cache(ttl_hours=int(http_cache_hours), enabled=bool(http_cache_enabled))

st.sidebar.markdown("---")

# ÙƒØ§Ø´ LLM
st.sidebar.subheader("ğŸ§  ÙƒØ§Ø´ LLM")
llm_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ LLM", value=True, key="opt_llm_cache_enabled")
llm_cache_hours   = st.sidebar.slider("TTL LLM Cache (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24, key="opt_llm_cache_hours")

if "llm_cacher" not in st.session_state:
    st.session_state["llm_cacher"] = LLMCacher(ttl_hours=int(llm_cache_hours))
else:
    st.session_state["llm_cacher"].ttl = int(llm_cache_hours) * 3600

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª LLM
st.sidebar.subheader("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡")
model_name = st.sidebar.selectbox(
    "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬",
    ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"],
    index=0,
    key="opt_model_name",
)
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.2, 0.1, key="opt_temperature")
max_tokens  = st.sidebar.slider("Max Tokens", 256, 8192, 2048, 64, key="opt_max_tokens")

st.sidebar.markdown("---")
st.sidebar.caption("ØªØ£ÙƒØ¯ Ù…Ù† ØªØ¹ÙŠÙŠÙ† `OPENAI_API_KEY` ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„ (Streamlit Cloud Secrets).")

# Main inputs
st.subheader("ğŸ§¾ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„")
col1, col2 = st.columns([2, 1])

with col1:
    topic  = st.text_input("Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹/Ø§Ù„ÙØ¦Ø©", key="inp_topic", placeholder="Ù…Ø«Ø§Ù„: Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¨Ø±Ø¬Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶")
    area   = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©/Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", key="inp_area")
    tone   = st.selectbox("Ø§Ù„Ù†Ø¨Ø±Ø© Ø§Ù„ÙƒØªØ§Ø¨ÙŠØ©", ["Ø§Ø­ØªØ±Ø§ÙÙŠØ©", "Ø­Ù…Ø§Ø³ÙŠØ©", "ÙˆØ¯Ù‘ÙŠØ©", "Ù…Ø®ØªØµØ±Ø©", "Ø³ÙŠØ§Ø­ÙŠØ©"], index=0, key="inp_tone")
    length = st.selectbox("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ", ["Ù‚ØµÙŠØ±", "Ù…ØªÙˆØ³Ø·", "Ø·ÙˆÙŠÙ„"], index=1, key="inp_length")

with col2:
    do_comp    = st.checkbox("ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†", value=False, key="opt_do_comp")
    do_quality = st.checkbox("ØªØ´ØºÙŠÙ„ ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø©", value=True, key="opt_do_quality")
    out_docx   = st.checkbox("ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù DOCX", value=True, key="opt_out_docx")
    out_json   = st.checkbox("ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù JSON", value=True, key="opt_out_json")

st.subheader("ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ù…ØµØ§Ø¯Ø± (ÙˆØ§Ø­Ø¯Ø© ÙÙŠ ÙƒÙ„ Ø³Ø·Ø±)")
urls_block = st.text_area("Ø£Ù„ØµÙ‚ Ø±ÙˆØ§Ø¨Ø· Ù…Ù‚Ø§Ù„Ø§Øª/Ù‚ÙˆØ§Ø¦Ù… Ù…Ø·Ø§Ø¹Ù…", height=120, key="inp_urls_block")
urls = parse_urls_block(urls_block)

st.subheader("ğŸ“ Ù…Ù„Ø§Ø­Ø¸Ø§Øª/ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
notes = st.text_area("Ø§ÙƒØªØ¨ Ø£ÙŠ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© ØªØ±ØºØ¨ ØªØ¶Ù…ÙŠÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„", height=100, key="inp_notes")

generate = st.button("ğŸš€ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¢Ù†", type="primary", key="btn_generate")

# ==========================
#       Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
# ==========================
def build_prompt(topic: str, area: str, tone: str, length: str, notes: str, sources: List[Dict[str, Any]]):
    sys_prompt = (
        "Ø£Ù†Øª Ù…Ø­Ø±Ù‘Ø± Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªØµ ÙÙŠ Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·Ø§Ø¹Ù…. Ø§ÙƒØªØ¨ Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ø¶Ø­Ø©ØŒ Ù…ÙØ­Ø§ÙŠØ¯Ø©ØŒ "
        "Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„Ø³ÙŠÙˆØŒ Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© ÙˆÙ†Ù‚Ø§Ø· Ø­ÙŠØ« ÙŠÙ„Ø²Ù…."
    )

    user_parts = []
    if topic:
        user_parts.append(f"Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: {topic}")
    if area:
        user_parts.append(f"Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ: {area}")
    user_parts.append(f"Ø§Ù„Ù†Ø¨Ø±Ø©: {tone}")
    user_parts.append(f"Ø§Ù„Ø·ÙˆÙ„: {length}")

    if notes.strip():
        user_parts.append(f"Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: {notes.strip()}")

    if sources:
        src_short = []
        for s in sources:
            title = (s.get("title") or "").strip()
            url = s.get("url") or ""
            txt  = (s.get("text") or "")[:1200]
            src_short.append({"title": title, "url": url, "snippet": txt})
        user_parts.append("Ù…Ù„Ø®Øµ Ù„Ù„Ù…ØµØ§Ø¯Ø± (Ø¹ÙŠÙ†Ø©):\n" + json.dumps(src_short, ensure_ascii=False, indent=2))

    return [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": "\n\n".join(user_parts)},
    ]

def call_llm(messages: List[Dict[str, Any]], model: str) -> str:
    cache_key = hash_messages(messages, model)
    if llm_cache_enabled:
        cached = st.session_state["llm_cacher"].get(cache_key)
        if cached:
            return cached

    resp = chat_complete_cached(messages=messages, model=model,
                                temperature=temperature, max_tokens=max_tokens)
    # Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ openai 1.x (Chat Completions)
    text = ""
    try:
        text = resp.choices[0].message.content  # type: ignore
    except Exception:
        text = str(resp)

    if llm_cache_enabled:
        st.session_state["llm_cacher"].set(cache_key, text)
    return text

def fetch_sources(urls: List[str]) -> List[Dict[str, Any]]:
    if not urls:
        return []
    req_id = str(uuid.uuid4())[:8]
    logger.info("restoguide | %s | app | places.fetch.start", req_id)
    out = []
    for u in urls:
        logger.info("restoguide | - | places | places.request")
        data = fetch_and_extract(u)
        out.append(data)
    # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª Ø­Ø³Ø¨ URL
    dedup = {}
    for row in out:
        dedup[row.get("url")] = row
    logger.info("restoguide | - | places | places.dedupe")
    rows = list(dedup.values())
    logger.info("restoguide | - | app | places.fetch.done")
    return rows

def export_outputs(article_text: str, meta: Dict[str, Any]):
    downloads = []
    if out_docx:
        docx_name = f"article_{int(time.time())}.docx"
        path = to_docx(article_text, docx_name)
        downloads.append(("DOCX", path))
    if out_json:
        json_name = f"article_{int(time.time())}.json"
        path = to_json(meta, json_name)
        downloads.append(("JSON", path))
    return downloads

# ==========================
#         Ø§Ù„ØªÙ†ÙÙŠØ°
# ==========================
if generate:
    if not topic.strip():
        st.error("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙˆØ¶ÙˆØ¹/ÙØ¦Ø© Ù„Ù„Ù…Ù‚Ø§Ù„.")
        st.stop()

    sources = fetch_sources(urls) if urls else []

    msgs = build_prompt(topic=topic, area=area, tone=tone, length=length, notes=notes, sources=sources)

    try:
        article_text = call_llm(msgs, model=model_name)
        logger.info("restoguide | - | app | places.accepted")
    except Exception as e:
        log_exception(e)
        # Ù„Ùˆ Ù…Ø§ ÙÙŠÙ‡ API Key â€” Ø¨Ù†Ø±Ø¬Ù‘Ø¹ Ù†Øµ Ø¨Ø¯ÙŠÙ„ Ø¨Ø¯Ù„ Ù…Ø§ ØªØªÙˆÙ‚Ù Ø§Ù„ØµÙØ­Ø©
        article_text = (
            "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ `OPENAI_API_KEY` Ø£Ùˆ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ API.\n\n"
            "â€” Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø¯ÙŠÙ„ â€”\n"
            f"Ø¹Ù†ÙˆØ§Ù†: {topic}\n"
            f"Ø§Ù„Ù…Ù†Ø·Ù‚Ø©: {area or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©'}\n"
            "Ù…Ù‚Ø¯Ù…Ø© Ù…Ø®ØªØµØ±Ø©â€¦\n\n"
            "â€¢ Ù…Ø·Ø¹Ù… 1: ÙˆØµÙ Ù…Ø®ØªØµØ±.\n"
            "â€¢ Ù…Ø·Ø¹Ù… 2: ÙˆØµÙ Ù…Ø®ØªØµØ±.\n"
            "â€¢ Ù…Ø·Ø¹Ù… 3: ÙˆØµÙ Ù…Ø®ØªØµØ±.\n\n"
            "Ø®Ø§ØªÙ…Ø© ÙˆÙ†ØµØ§Ø¦Ø­ Ù„Ù„Ø­Ø¬Ø² ÙˆØ£ÙˆÙ‚Ø§Øª Ø§Ù„Ø°Ø±ÙˆØ©â€¦"
        )

    parsed, plain = normalize_json_or_text(article_text)
    final_text = ""
    meta: Dict[str, Any] = {
        "topic": topic, "area": area, "tone": tone, "length": length, "notes": notes,
        "sources_count": len(sources), "model": model_name,
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    }

    if isinstance(parsed, dict) and parsed.get("article"):
        final_text = str(parsed.get("article"))
        meta["structure"] = "json"
        meta["raw"] = parsed
    else:
        final_text = plain or article_text
        meta["structure"] = "text"

    if do_comp and sources:
        comp = analyze_competitors([s.get("text", "") for s in sources])
        meta["competitor_analysis"] = comp
        meta["content_gaps"] = extract_gap(comp)

    if do_quality:
        meta["quality_report"] = quality_report(final_text)

    st.subheader("âœï¸ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬")
    st.text_area("Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ", value=final_text, height=400, key="out_final_article")

    st.subheader("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„Ø§Øª")
    downloads = export_outputs(final_text, meta)
    if not downloads:
        st.info("Ù„Ù… ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ Ø£ÙŠ Ø®ÙŠØ§Ø± ØªÙ†Ø²ÙŠÙ„.")
    else:
        for kind, path in downloads:
            st.markdown(f"- **{kind}**: [ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù]({path})")

st.markdown("---")
st.caption("Â© 2025 RestoGuide â€” ÙŠØ¹Ù…Ù„ Ø¨ÙˆØ§Ø³Ø·Ø© Streamlit Ùˆ OpenAI. ")
