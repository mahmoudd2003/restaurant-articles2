# app.py â€” Ù†Ø³Ø®Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ø­Ø¯Ø«Ø©
# ===============================================================
# Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
# - ÙŠØªØ¶Ù…Ù†: ÙƒØ§Ø´ HTTP + ÙƒØ§Ø´ LLM + ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ø²Ø§Ù…ÙŠØ© + FAQ Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ + JSON-LD
# - ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ prompts ÙˆÙ…Ù„ÙØ§ØªÙ‡: base.md, polish.md, faq.md, methodology.md, criteria_*.md
# - ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ data/criteria_catalog.yaml Ø¥Ù† ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… get_category_criteria
# ===============================================================

import os
import io
import csv
import json
import unicodedata
from datetime import datetime
from pathlib import Path

import streamlit as st

# ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
os.makedirs("data", exist_ok=True)

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¯Ø§Ø®Ù„ÙŠØ© ---
from utils.content_fetch import fetch_and_extract, configure_http_cache, clear_http_cache
try:
    # Ø­Ø³Ø¨ Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ø¹Ù†Ø¯Ùƒ
    from category_criteria import get_category_criteria
except ImportError:
    # ÙÙŠ Ø­Ø§Ù„ Ù†Ù‚Ù„ØªÙ‡ Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ modules
    from modules.category_criteria import get_category_criteria

from utils.openai_client import get_client, chat_complete_cached
from utils.exporters import to_docx, to_json
from utils.competitor_analysis import analyze_competitors, extract_gap_points
from utils.quality_checks import quality_report
from utils.llm_reviewer import llm_review, llm_fix
from utils.llm_cache import LLMCacher
from utils.keywords import parse_required_keywords, enforce_report, FIX_PROMPT
from utils.references import normalize_refs, build_references_md, build_citation_map

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Ù…ÙˆÙ„Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (E-E-A-T)", page_icon="ğŸ½ï¸", layout="wide")
st.title("ğŸ½ï¸ Ù…ÙˆÙ„Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… â€” E-E-A-T + Human Touch + Ù…Ù†Ø§ÙØ³ÙŠÙ† + ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ø²Ø§Ù…ÙŠØ© + Ù…Ø±Ø§Ø¬Ø¹ + QC")

# --- Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ---
def safe_rerun():
    if getattr(st, "rerun", None):
        st.rerun()  # Streamlit >= 1.30
    else:
        st.experimental_rerun()

def _has_api_key() -> bool:
    try:
        if hasattr(st, "secrets") and "OPENAI_API_KEY" in st.secrets and st.secrets["OPENAI_API_KEY"]:
            return True
    except Exception:
        pass
    return bool(os.getenv("OPENAI_API_KEY"))

def slugify(name: str) -> str:
    s = ''.join(c for c in unicodedata.normalize('NFKD', name) if not unicodedata.combining(c))
    import re as _re
    s = _re.sub(r'\W+', '_', s).strip('_').lower()
    return s or "custom"

PROMPTS_DIR = Path("prompts")
def read_prompt(name: str) -> str:
    return (PROMPTS_DIR / name).read_text(encoding="utf-8")

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
BASE_TMPL = read_prompt("base.md")
POLISH_TMPL = read_prompt("polish.md")
FAQ_TMPL = read_prompt("faq.md")
METH_TMPL = read_prompt("methodology.md")
CRITERIA_MAP = {
    "Ø¨ÙŠØªØ²Ø§": read_prompt("criteria_pizza.md"),
    "Ù…Ù†Ø¯ÙŠ": read_prompt("criteria_mandy.md"),
    "Ø¨Ø±Ø¬Ø±": read_prompt("criteria_burger.md"),
    "ÙƒØ§ÙÙŠÙ‡Ø§Øª": read_prompt("criteria_cafes.md"),
}
GENERAL_CRITERIA = read_prompt("criteria_general.md")

# --- Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ù…ÙƒØ§Ù† ---
PLACE_TEMPLATES = {
    "Ù…ÙˆÙ„/Ù…Ø¬Ù…Ø¹": "Ø§Ø­Ø¬Ø² Ù‚Ø¨Ù„ Ø§Ù„Ø°Ø±ÙˆØ© Ø¨Ù€20â€“30 Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø±Ø§Ù‚Ø¨ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ø±ÙˆØ¶/Ø§Ù„Ù†Ø§ÙÙˆØ±Ø©ØŒ ÙˆØªØ¬Ù†Ù‘Ø¨ Ø·ÙˆØ§Ø¨ÙŠØ± Ø§Ù„Ù…ØµØ§Ø¹Ø¯.",
    "Ø¬Ù‡Ø© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© (Ø´Ù…Ø§Ù„/Ø´Ø±Ù‚..)": "Ø§Ù„ÙˆØµÙˆÙ„ Ø£Ø³Ù‡Ù„ Ø¹Ø¨Ø± Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ© Ù‚Ø¨Ù„ 7:30Ù…ØŒ Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ø´ÙˆØ§Ø±Ø¹ Ù‚Ø¯ ØªÙ…ØªÙ„Ø¦ Ù…Ø¨ÙƒØ±Ù‹Ø§ ÙÙŠ Ø§Ù„ÙˆÙŠÙƒÙ†Ø¯.",
    "Ø­ÙŠÙ‘ Ù…Ø­Ø¯Ø¯": "Ø§Ù„Ù…Ø´ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ø´Ø§Ø¡ Ø®ÙŠØ§Ø± Ù„Ø·ÙŠÙ Ø¥Ù† ØªÙˆÙÙ‘Ø±Øª Ø£Ø±ØµÙØ© Ù‡Ø§Ø¯Ø¦Ø©ØŒ Ø§Ù†ØªØ¨Ù‡ Ù„Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø°Ø±ÙˆØ© Ø¨ÙŠÙ† Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ§Ù„ÙˆÙŠÙƒÙ†Ø¯.",
    "Ø´Ø§Ø±Ø¹/Ù…Ù…Ø´Ù‰": "Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø£Ù„Ø·Ù Ø¨Ø¹Ø¯ Ø§Ù„Ù…ØºØ±Ø¨ ØµÙŠÙÙ‹Ø§ØŒ ÙˆØ§Ù„Ø¨Ø±Ø¯ Ø§Ù„Ù„ÙŠÙ„ÙŠ Ù‚Ø¯ ÙŠØªØ·Ù„Ù‘Ø¨ Ù…Ø´Ø±ÙˆØ¨Ù‹Ø§ Ø³Ø§Ø®Ù†Ù‹Ø§ Ø´ØªØ§Ø¡Ù‹.",
    "ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø­Ø±ÙŠØ©/ÙƒÙˆØ±Ù†ÙŠØ´": "Ø§Ù„Ù‡ÙˆØ§Ø¡ Ø£Ù‚ÙˆÙ‰ Ù…Ø³Ø§Ø¡Ù‹â€”Ø§Ø·Ù„Ø¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø³Ø±ÙŠØ¹Ù‹Ø§ ÙˆÙŠÙÙØ¶Ù‘Ù„ Ø§Ù„Ù…Ù‚Ø§Ø¹Ø¯ Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø© Ø¹Ù† Ø§Ù„ØªÙŠØ§Ø±Ø§Øª.",
    "ÙÙ†Ø¯Ù‚/Ù…Ù†ØªØ¬Ø¹": "Ù‚Ø¯ ØªØ±ØªÙØ¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù„ÙƒÙ† Ø§Ù„Ø®Ø¯Ù…Ø© Ø£Ø¯Ù‚Ù‘ØŒ Ø§Ø­Ø¬Ø² Ø¨Ø§ÙƒØ±Ù‹Ø§ Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ù†ÙˆØ§ÙØ°/Ø§Ù„Ø¥Ø·Ù„Ø§Ù„Ø§Øª.",
    "Ù…Ø¯ÙŠÙ†Ø© ÙƒØ§Ù…Ù„Ø©": "ÙØ±ÙˆØ¹ Ø³Ù„Ø³Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù‚Ø¯ ØªØ®ØªÙ„Ù Ø¬ÙˆØ¯ØªÙ‡Ø§ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ØŒ Ø§Ø·Ù„Ø¨ Ø§Ù„Ø·Ø¨Ù‚ Ø§Ù„Ø£Ø´Ù‡Ø± Ø£ÙˆÙ„Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªÙˆÙ‰."
}
def build_protip_hint(place_type: str) -> str:
    return PLACE_TEMPLATES.get(place_type or "", "Ù‚Ø¯Ù‘Ù… Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ù„Ø°Ø±ÙˆØ© ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ÙˆØµÙˆÙ„.")

def build_place_context(place_type: str, place_name: str, place_rules: str, strict: bool) -> str:
    scope = "ØµØ§Ø±Ù… (Ø§Ù„ØªØ²Ù… Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚ ÙÙ‚Ø·)" if strict else "Ù…Ø±Ù† (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚)"
    return f"""Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ÙƒØ§Ù†:
- Ø§Ù„Ù†ÙˆØ¹: {place_type or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"}
- Ø§Ù„Ø§Ø³Ù…: {place_name or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"}
- Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚: {place_rules or "â€”"}
- ØµØ±Ø§Ù…Ø© Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚: {scope}"""

# ========================= Sidebar =========================
st.sidebar.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©")
tone = st.sidebar.selectbox(
    "Ù†ØºÙ…Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨",
    ["Ù†Ø§Ù‚Ø¯ ÙˆØ¯ÙˆØ¯", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù…", "Ø¯Ù„ÙŠÙ„ ØªØ­Ø±ÙŠØ±ÙŠ Ù…Ø­Ø§ÙŠØ¯", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª"]
)
primary_model = st.sidebar.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ", ["gpt-4.1", "gpt-4o", "gpt-4o-mini"], index=0)
fallback_model = st.sidebar.selectbox("Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¯ÙŠÙ„ (Fallback)", ["gpt-4o", "gpt-4o-mini", "gpt-4.1"], index=1)
include_faq = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© Ù‚Ø³Ù… FAQ", value=True)
include_methodology = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ø±ÙŠØ±", value=True)
add_human_touch = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ Ø·Ø¨Ù‚Ø© Ø§Ù„Ù„Ù…Ø³Ø§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (Polish)", value=True)
approx_len = st.sidebar.slider("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ (ÙƒÙ„Ù…Ø§Øª)", 600, 1800, 1100, step=100)

review_weight = None
if tone in ["Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª"]:
    default_weight = 85 if tone == "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±" else 55
    review_weight = st.sidebar.slider("ÙˆØ²Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª (Ùª)", 0, 100, default_weight, step=5)

# Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ§© ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¥Ù„Ø²Ø§Ù…ÙŠØ©")
kw_help = "Ø§ÙƒØªØ¨ ÙƒÙ„ ÙƒÙ„Ù…Ø©/Ø¹Ø¨Ø§Ø±Ø© Ø¨Ø³Ø·Ø± Ù…Ø³ØªÙ‚Ù„. Ù„Ø¥Ø¬Ø¨Ø§Ø± ØªÙƒØ±Ø§Ø±Ù‡Ø§ Ø¶Ø¹ | min=2 Ù…Ø«Ù„: Ø¬Ù„Ø³Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ© | min=2"
required_kw_spec = st.sidebar.text_area("Ø£Ø¯Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©", value="Ù…Ø·Ø§Ø¹Ù… Ø¹Ø§Ø¦Ù„ÙŠØ©\nØ¬Ù„Ø³Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ© | min=2", height=120, help=kw_help)
required_list = parse_required_keywords(required_kw_spec)

# Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
internal_catalog = st.sidebar.text_area(
    "Ø£Ø¯Ø®Ù„ Ø¹Ù†Ø§ÙˆÙŠÙ†/Ø³Ù„Ø§Ú¯Ø² Ù…Ù‚Ø§Ù„Ø§ØªÙƒ (Ø³Ø·Ø± Ù„ÙƒÙ„ Ø¹Ù†ØµØ±)",
    "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ø±ÙŠØ§Ø¶\nØ£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¥ÙØ·Ø§Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶\nØ£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¨ÙŠØªØ²Ø§ ÙÙŠ Ø¬Ø¯Ø©"
)

# Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© + Ø§Ù„Ù…Ø¤Ù„Ù/Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“š Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©")
refs_text = st.sidebar.text_area(
    "Ø±ÙˆØ§Ø¨Ø· Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø© (Ø³Ø·Ø± Ù„ÙƒÙ„ Ø±Ø§Ø¨Ø·)",
    value="https://goo.gl/maps/\nhttps://www.timeoutdubai.com/\nhttps://www.michelin.com/",
    height=120,
    help="Ø£Ø¯Ø®Ù„ Ø±ÙˆØ§Ø¨Ø· ØµÙØ­Ø§Øª/ØªÙ‚Ø§Ø±ÙŠØ±/Ø£Ø¯Ù„Ø© Ù…ÙˆØ«ÙˆÙ‚Ø© Ù„Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù‡Ø§."
)
author_name = st.sidebar.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ù„Ù/Ø§Ù„Ù…Ø­Ø±Ø±", value="ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ­Ø±ÙŠØ±")
reviewer_name = st.sidebar.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ø±Ø§Ø¬ÙØ¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", value="")
last_verified = st.sidebar.text_input("ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± ØªØ­Ù‚Ù‚ (YYYY-MM-DD)", value=datetime.now().strftime("%Y-%m-%d"))

# ÙƒØ§Ø´ HTTP
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ§  Ø§Ù„ÙƒØ§Ø´ (Ø¬Ù„Ø¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·)")
use_cache = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ HTTP", value=True, help="ÙŠÙØ³Ø±Ù‘Ø¹ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø§Øª ÙˆÙŠÙ‚Ù„Ù‘Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©.")
cache_hours = st.sidebar.slider("Ù…Ø¯Ø© ÙƒØ§Ø´ HTTP (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24)
if st.sidebar.button("ğŸ§¹ Ù…Ø³Ø­ ÙƒØ§Ø´ HTTP"):
    ok = clear_http_cache()
    st.sidebar.success("ØªÙ… Ù…Ø³Ø­ ÙƒØ§Ø´ HTTP." if ok else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ø´.")
try:
    configure_http_cache(enabled=use_cache, hours=cache_hours)
except Exception as e:
    st.sidebar.warning(f"ØªØ¹Ø°Ù‘Ø± ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´ HTTP: {e}")

# ÙƒØ§Ø´ LLM
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ§  ÙƒØ§Ø´ Ø§Ù„Ù€LLM")
llm_cache_enabled = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ ÙƒØ§Ø´ Ù…Ø®Ø±Ø¬Ø§Øª LLM", value=True, help="ÙŠÙ‚Ù„Ù„ Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„ØªÙƒÙ„ÙØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ·ÙˆÙŠØ±.")
llm_cache_hours = st.sidebar.slider("Ù…Ø¯Ø© ÙƒØ§Ø´ LLM (Ø³Ø§Ø¹Ø§Øª)", 1, 72, 24)
if "llm_cacher" not in st.session_state:
    st.session_state["llm_cacher"] = LLMCacher(ttl_hours=llm_cache_hours, enabled=llm_cache_enabled)
else:
    st.session_state["llm_cacher"].configure(enabled=llm_cache_enabled, ttl_hours=llm_cache_hours)
if st.sidebar.button("ğŸ§¹ Ù…Ø³Ø­ ÙƒØ§Ø´ LLM"):
    ok = st.session_state["llm_cacher"].clear()
    st.sidebar.success("ØªÙ… Ù…Ø³Ø­ ÙƒØ§Ø´ LLM." if ok else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ø´.")

# ========================= Tabs =========================
tab_article, tab_comp, tab_qc = st.tabs(["âœï¸ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„", "ğŸ†š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ† (Ø±ÙˆØ§Ø¨Ø· ÙŠØ¯ÙˆÙŠØ©)", "ğŸ§ª ÙØ­Øµ Ø¨Ø´Ø±ÙŠØ© ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"])

# ------------------ Tab 1: Article Generation ------------------
with tab_article:
    col1, col2 = st.columns([2,1])
    with col1:
        article_title = st.text_input("Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„", "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶")
        keyword = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", "Ù…Ø·Ø§Ø¹Ù… ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶")

        COUNTRIES = {
            "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©": ["Ø§Ù„Ø±ÙŠØ§Ø¶","Ø¬Ø¯Ø©","Ù…ÙƒØ©","Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©","Ø§Ù„Ø¯Ù…Ø§Ù…","Ø§Ù„Ø®Ø¨Ø±","Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†","Ø§Ù„Ø·Ø§Ø¦Ù","Ø£Ø¨Ù‡Ø§","Ø®Ù…ÙŠØ³ Ù…Ø´ÙŠØ·","Ø¬Ø§Ø²Ø§Ù†","Ù†Ø¬Ø±Ø§Ù†","ØªØ¨ÙˆÙƒ","Ø¨Ø±ÙŠØ¯Ø©","Ø¹Ù†ÙŠØ²Ø©","Ø§Ù„Ù‡ÙÙˆÙ","Ø§Ù„Ø£Ø­Ø³Ø§Ø¡","Ø§Ù„Ø¬Ø¨ÙŠÙ„","Ø§Ù„Ù‚Ø·ÙŠÙ","ÙŠÙ†Ø¨Ø¹","Ø­Ø§Ø¦Ù„"],
            "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª": ["Ø¯Ø¨ÙŠ","Ø£Ø¨ÙˆØ¸Ø¨ÙŠ","Ø§Ù„Ø´Ø§Ø±Ù‚Ø©","Ø¹Ø¬Ù…Ø§Ù†","Ø±Ø£Ø³ Ø§Ù„Ø®ÙŠÙ…Ø©","Ø§Ù„ÙØ¬ÙŠØ±Ø©","Ø£Ù… Ø§Ù„Ù‚ÙŠÙˆÙŠÙ†","Ø§Ù„Ø¹ÙŠÙ†"]
        }
        country = st.selectbox("Ø§Ù„Ø¯ÙˆÙ„Ø©", ["Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª", "Ø£Ø®Ø±Ù‰â€¦"], index=0)
        if country in COUNTRIES:
            city_choice = st.selectbox("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", COUNTRIES[country] + ["Ù…Ø¯ÙŠÙ†Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦"], index=0)
            city_input = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", city_choice) if city_choice == "Ù…Ø¯ÙŠÙ†Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦" else city_choice
        else:
            country = st.text_input("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆÙ„Ø©", "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©")
            city_input = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", "Ø§Ù„Ø±ÙŠØ§Ø¶")

        place_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ù…ÙƒØ§Ù†",
            ["Ù…ÙˆÙ„/Ù…Ø¬Ù…Ø¹", "Ø¬Ù‡Ø© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© (Ø´Ù…Ø§Ù„/Ø´Ø±Ù‚..)", "Ø­ÙŠÙ‘ Ù…Ø­Ø¯Ø¯", "Ø´Ø§Ø±Ø¹/Ù…Ù…Ø´Ù‰", "ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø­Ø±ÙŠØ©/ÙƒÙˆØ±Ù†ÙŠØ´", "ÙÙ†Ø¯Ù‚/Ù…Ù†ØªØ¬Ø¹", "Ù…Ø¯ÙŠÙ†Ø© ÙƒØ§Ù…Ù„Ø©"], index=0)
        place_name = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…ÙƒØ§Ù†/Ø§Ù„Ù†Ø·Ø§Ù‚", placeholder="Ù…Ø«Ù„Ù‹Ø§: Ø¯Ø¨ÙŠ Ù…ÙˆÙ„ / Ø´Ù…Ø§Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶")
        place_rules = st.text_area("Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", placeholder="Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙˆÙ„ ÙÙ‚Ø·ØŒ Ø£Ùˆ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡: Ø§Ù„Ø±Ø¨ÙŠØ¹/Ø§Ù„ÙŠØ§Ø³Ù…ÙŠÙ†/Ø§Ù„Ù…Ø±ÙˆØ¬â€¦", height=80)
        strict_in_scope = st.checkbox("Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ ÙÙ‚Ø· (ØµØ§Ø±Ù…)", value=True)

        content_scope = st.radio("Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰", ["ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØ§Ù†", "Ø´Ø§Ù…Ù„ Ø¨Ù„Ø§ ÙØ¦Ø©", "Ù‡Ø¬ÙŠÙ† (ØªÙ‚Ø³ÙŠÙ… Ø¯Ø§Ø®Ù„ÙŠ)"], index=1 if place_type=="Ù…ÙˆÙ„/Ù…Ø¬Ù…Ø¹" else 0)

        built_in_labels = list(CRITERIA_MAP.keys())
        category = "Ø¹Ø§Ù…"
        criteria_block = GENERAL_CRITERIA

        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙØ¦Ø©
        is_custom_category = False
        if content_scope == "ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØ§Ù†":
            category_choice = st.selectbox("Ø§Ù„ÙØ¦Ø©", built_in_labels + ["ÙØ¦Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦"])
            if category_choice == "ÙØ¦Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦":
                if "pending_custom_criteria_text" in st.session_state:
                    st.session_state["custom_criteria_text"] = st.session_state.pop("pending_custom_criteria_text")
                custom_category_name = st.text_input("Ø§Ø³Ù… Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø®ØµÙ‘ØµØ©", "Ù…Ø·Ø§Ø¹Ù… Ù„Ø¨Ù†Ø§Ù†ÙŠØ©", key="custom_category_name")
                DEFAULT_CRIT_MD = (
                    "- **Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©:** Ø²ÙŠØ§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ù‘Ø¯Ø© ÙˆØªØ¬Ø±Ø¨Ø© Ø£Ø·Ø¨Ø§Ù‚ Ø£Ø³Ø§Ø³ÙŠØ©.\n"
                    "- **Ø§Ù„Ù…ÙƒÙˆÙ‘Ù†Ø§Øª:** Ø¬ÙˆØ¯Ø© ÙˆØ·Ø²Ø§Ø¬Ø©.\n"
                    "- **Ø§Ù„Ø£ØµØ§Ù„Ø©/Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:** Ø§Ù„ØªØªØ¨ÙŠÙ„/Ø§Ù„Ø´ÙˆÙŠ/Ø§Ù„ÙØ±Ù† ÙˆÙ…Ø¯Ù‰ Ù‚Ø±Ø¨ Ø§Ù„Ù†ÙƒÙ‡Ø© Ù…Ù† Ø§Ù„Ø£ØµÙ„.\n"
                    "- **Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡:** Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ø¹Ø§Ø¦Ù„Ø§Øª/Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡.\n"
                    "- **Ø«Ø¨Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©:** Ø¹Ø¨Ø± Ø£ÙˆÙ‚Ø§Øª/Ø²ÙŠØ§Ø±Ø§Øª Ù…Ø®ØªÙ„ÙØ©."
                )
                ta_kwargs = dict(key="custom_criteria_text", height=140)
                if "custom_criteria_text" not in st.session_state:
                    ta_kwargs["value"] = DEFAULT_CRIT_MD
                custom_criteria_text = st.text_area("Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", **ta_kwargs)
                category = (st.session_state.get("custom_category_name") or "ÙØ¦Ø© Ù…Ø®ØµÙ‘ØµØ©").strip()
                criteria_block = st.session_state.get("custom_criteria_text") or "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø©ØŒ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§ØªØŒ ØªÙ†ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ ÙˆØ«Ø¨Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©."
                is_custom_category = True
            else:
                category = category_choice
                criteria_block = CRITERIA_MAP.get(category_choice, GENERAL_CRITERIA)
        else:
            category = "Ø¹Ø§Ù…"
            criteria_block = GENERAL_CRITERIA

        # Ø²Ø±/Ø®ÙŠØ§Ø± Ø¬Ù„Ø¨/ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ÙØ¦Ø©
        def _normalize_criteria(raw):
            if raw is None: return []
            if isinstance(raw, str):
                s = raw.strip()
                import json as _json
                if s.startswith(("[","{"])):
                    try: raw = _json.loads(s)
                    except Exception:
                        lines = [ln.strip(" -â€¢\t").strip() for ln in s.splitlines() if ln.strip()]
                        return [ln for ln in lines if ln and ln.lower()!="undefined"]
                else:
                    lines = [ln.strip(" -â€¢\t").strip() for ln in s.splitlines() if ln.strip()]
                    return [ln for ln in lines if ln and ln.lower()!="undefined"]
            if isinstance(raw, dict):
                for k in ("criteria","bullets","items","list"):
                    if k in raw: raw = raw[k]; break
                else:
                    vals = list(raw.values())
                    raw = vals if all(isinstance(v,str) for v in vals) else list(raw.keys())
            if isinstance(raw, (list,tuple)):
                out=[]
                for x in raw:
                    if isinstance(x,str): t=x.strip().strip(",").strip('"').strip("'")
                    elif isinstance(x,dict) and "text" in x: t=str(x["text"]).strip()
                    else: t=str(x).strip()
                    if t and t.lower()!="undefined": out.append(t)
                return out
            return [str(raw)]
        def _format_criteria_md(items):
            items = _normalize_criteria(items)
            return "\n".join(f"- {c}" for c in items) or "- â€”"

        effective_category = (category or "Ø¹Ø§Ù…").strip()
        if "criteria_generated_md_map" not in st.session_state:
            st.session_state["criteria_generated_md_map"] = {}

        with st.expander("ğŸ“‹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø© (ØªÙ„Ù‚Ø§Ø¦ÙŠ/ÙŠØ¯ÙˆÙŠ)", expanded=False):
            st.caption(f"Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: **{effective_category}**")
            use_llm = st.checkbox("ØªØ¹Ø²ÙŠØ² Ø¨Ø§Ù„Ù€ LLM (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", value=False, key="crit_llm")
            if st.button("Ø¬Ù„Ø¨/ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ÙØ¦Ø©", key="btn_generate_criteria"):
                crit_list = get_category_criteria(effective_category, use_llm=use_llm, catalog_path="data/criteria_catalog.yaml")
                md = _format_criteria_md(crit_list)
                st.session_state["criteria_generated_md_map"].pop(effective_category, None)
                st.session_state["criteria_generated_md_map"][effective_category] = md
                if is_custom_category:
                    st.session_state["pending_custom_criteria_text"] = md
                    safe_rerun()
                else:
                    st.success("ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ÙˆØ­ÙØ¸Ù‡Ø§.")
            if effective_category in st.session_state["criteria_generated_md_map"]:
                st.markdown("**Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± (ØªÙ„Ù‚Ø§Ø¦ÙŠ):**")
                st.markdown(st.session_state["criteria_generated_md_map"][effective_category])

        if is_custom_category:
            criteria_block = st.session_state.get("custom_criteria_text", criteria_block)
        else:
            criteria_block = st.session_state.get("criteria_generated_md_map", {}).get(effective_category, criteria_block)

        restaurants_input = st.text_area("Ø£Ø¯Ø®Ù„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (Ø³Ø·Ø± Ù„ÙƒÙ„ Ù…Ø·Ø¹Ù…)", "Ù…Ø·Ø¹Ù… 1\nÙ…Ø·Ø¹Ù… 2\nÙ…Ø·Ø¹Ù… 3", height=160)
        st.markdown("**Ø£Ùˆ** Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (Ø¹Ù…ÙˆØ¯: name)")
        csv_file = st.file_uploader("Ø±ÙØ¹ CSV (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", type=["csv"])

        def _normalize_name(s: str) -> str:
            return " ".join((s or "").strip().split())
        def _merge_unique(a: list, b: list) -> list:
            seen, out = set(), []
            for x in a + b:
                x2 = _normalize_name(x)
                if x2 and x2 not in seen:
                    seen.add(x2); out.append(x2)
            return out

        typed_restaurants = [r.strip() for r in restaurants_input.splitlines() if r.strip()]
        uploaded_restaurants = []
        if csv_file:
            try:
                text = csv_file.read().decode("utf-8-sig")
                reader = csv.DictReader(io.StringIO(text))
                for row in reader:
                    name = row.get("name") or row.get("Ø§Ø³Ù…") or ""
                    if name.strip():
                        uploaded_restaurants.append(name.strip())
            except Exception as e:
                st.warning(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© CSV: {e}")
        restaurants = _merge_unique(typed_restaurants, uploaded_restaurants)

        manual_notes = st.text_area("Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙŠØ¯ÙˆÙŠØ© ØªÙØ¯Ù…Ø¬ Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", st.session_state.get("comp_gap_notes",""))

    with col2:
        st.subheader("Ù‚Ø§Ø¦Ù…Ø© ØªØ¯Ù‚ÙŠÙ‚ Ø¨Ø´Ø±ÙŠØ©")
        checks = {
            "sensory": st.checkbox("Ø£Ø¶Ù ÙˆØµÙÙ‹Ø§ Ø­Ø³ÙŠÙ‹Ø§ Ø¯Ù‚ÙŠÙ‚Ù‹Ø§ (Ø±Ø§Ø¦Ø­Ø©/Ù‚ÙˆØ§Ù…/Ø­Ø±Ø§Ø±Ø©) Ù„Ù…Ø·Ø¹Ù… ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„"),
            "personal": st.checkbox("Ø£Ø¯Ø±Ø¬ Ù…Ù„Ø§Ø­Ø¸Ø© Ø´Ø®ØµÙŠØ©/ØªÙØ¶ÙŠÙ„ Ø´Ø®ØµÙŠ"),
            "compare": st.checkbox("Ø£Ø¶Ù Ù…Ù‚Ø§Ø±Ù†Ø© ØµØºÙŠØ±Ø© Ù…Ø¹ Ø²ÙŠØ§Ø±Ø© Ø³Ø§Ø¨Ù‚Ø©/Ù…Ø·Ø¹Ù… Ù…Ø´Ø§Ø¨Ù‡"),
            "critique": st.checkbox("Ø£Ø¶Ù Ù†Ù‚Ø¯Ù‹Ø§ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ (ØªÙØµÙŠÙ„Ø© Ø³Ù„Ø¨ÙŠØ© ØµØºÙŠØ±Ø©)"),
            "vary": st.checkbox("Ù†ÙˆÙ‘Ø¹ Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø±ØªØ§Ø¨Ø©"),
        }

    if st.button("ğŸš€ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„"):
        if not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
            st.stop()
        client = get_client()

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø¨Ø±Ø©
        if tone == "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±":
            tone_instructions = ("Ø§ÙƒØªØ¨ ÙƒÙ†Ù‘Ø§Ù‚Ø¯ ØµØ§Ø±Ù… ÙŠØ¹ØªÙ…Ø¯ Ø£Ø³Ø§Ø³Ù‹Ø§ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø© Ø¹Ù„Ù†Ù‹Ø§. "
                                 "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© ÙˆØ§Ø°ÙƒØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©. Ù„Ø§ ØªØ¯Ù‘Ø¹Ù Ø²ÙŠØ§Ø±Ø© Ø´Ø®ØµÙŠØ©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£Ø±Ù‚Ø§Ù….")
            tone_selection_line = "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© Ù…Ù†Ø´ÙˆØ±Ø© Ø¹Ù„Ù†Ù‹Ø§ Ø­ØªÙ‰ {last_updated}ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©."
            system_tone = "Ø£Ø³Ù„ÙˆØ¨ Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… Ù…Ø±ØªÙƒØ² Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±"
        elif tone == "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª":
            tone_instructions = ("Ø§ÙƒØªØ¨ ÙƒÙ†Ù‘Ø§Ù‚Ø¯ ØµØ§Ø±Ù… ÙŠÙ…Ø²Ø¬ Ø®Ø¨Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ© Ù…Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±. "
                                 "Ù‚Ø¯Ù‘Ù… Ø§Ù„Ø­ÙƒÙ… Ù…Ù† Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø£ÙˆÙ„Ù‹Ø§ Ø«Ù… Ù‚Ø§Ø±Ù†Ù‡ Ø¨Ø§Ù†Ø·Ø¨Ø§Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±. Ø£Ø¯Ø±Ø¬ **Ù†Ù‚Ø·Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†** Ù„ÙƒÙ„ Ù…Ø·Ø¹Ù….")
            tone_selection_line = "Ù…Ø²Ø¬Ù†Ø§ Ø¨ÙŠÙ† Ø²ÙŠØ§Ø±Ø§Øª Ù…ÙŠØ¯Ø§Ù†ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨ ÙØ¹Ù„ÙŠØ© ÙˆÙ…Ø±Ø§Ø¬Ø¹Ø§Øª Ø¹Ø§Ù…Ø© Ø­ØªÙ‰ {last_updated}."
            system_tone = "Ø£Ø³Ù„ÙˆØ¨ Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… ÙŠÙ…Ø²Ø¬ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±"
        else:
            tone_instructions = "Ø§ÙƒØªØ¨ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…ØªÙˆØ§Ø²Ù† ÙŠØ±Ø§Ø¹ÙŠ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ÙˆØ¶ÙˆØ­ Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ©."
            tone_selection_line = "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© Ù…ØªØ§Ø­Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯ÙˆØ±ÙŠØ©."
            system_tone = tone

        # Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if content_scope == "ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØ§Ù†":
            scope_instructions = "Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ."
        elif content_scope == "Ù‡Ø¬ÙŠÙ† (ØªÙ‚Ø³ÙŠÙ… Ø¯Ø§Ø®Ù„ÙŠ)":
            scope_instructions = "Ù‚Ø³Ù‘Ù… Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙˆØ§Ø²Ù† Ø§Ù„ØªÙ†ÙˆØ¹."
        else:
            scope_instructions = "Ù‚Ø¯Ù‘Ù… ØªØ´ÙƒÙŠÙ„Ø© Ù…ØªÙ†ÙˆØ¹Ø© ØªÙ…Ø«Ù‘Ù„ Ø§Ù„Ù…ÙƒØ§Ù†."

        protip_hint = build_protip_hint(place_type)
        place_context = build_place_context(place_type, place_name, place_rules, strict_in_scope)

        # FAQ Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        if include_faq:
            faq_prompt = (
                "Ø§ÙƒØªØ¨ 5â€“8 Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© ÙˆØ¥Ø¬Ø§Ø¨Ø§Øª Ù‚ØµÙŠØ±Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\n"
                "Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Øª Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…ØµØ¯Ø±ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø­Ø§Ø´ÙŠØ© [^n] Ø¨Ø±Ù‚Ù… Ù…Ø±Ø¬Ø¹ÙŠ Ù…Ù† Ù‚Ø³Ù… \"Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹\" Ø£Ø¯Ù†Ø§Ù‡.\n"
                f"Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©/Ø§Ù„Ù…ÙƒØ§Ù†: {place_name or city_input}\n"
                f"Ø§Ù„ÙØ¦Ø©: {category}\n"
                "Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ù‚ØªØ±Ø­Ø©: Ø§Ù„Ø­Ø¬Ø²/Ø§Ù„Ø°Ø±ÙˆØ©/Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©/Ø§Ù„Ø¹Ø§Ø¦Ù„Ø§Øª/Ø§Ù„Ù„Ø¨Ø§Ø³/Ø§Ù„Ù…ÙˆØ§Ù‚Ù/Ø·Ø±Ù‚ Ø§Ù„Ø¯ÙØ¹.\n"
                "Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ù…ØµØ§Ø¯Ø±. Ø¥Ù† ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ù…Ù† Ø®Ø¨Ø±Ø© ØªØ­Ø±ÙŠØ±ÙŠØ©ØŒ Ø§Ø°ÙƒØ± (Ø®Ø¨Ø±Ø© ØªØ­Ø±ÙŠØ±ÙŠØ©) Ø¨Ø¯Ù„ Ø§Ù„Ø­Ø§Ø´ÙŠØ©.\n"
                "(Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø³ØªØ¸Ù‡Ø± Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù‚Ø§Ù„Ø› Ø±Ù‚Ù‘Ù… Ø§Ù„Ø­ÙˆØ§Ø´ÙŠ ÙÙ‚Ø· Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·.)"
            )
            faq_messages = [
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ Ù…Ø­ØªØ±Ù ÙŠÙƒØªØ¨ FAQ Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆÙ…Ø¯Ø¹ÙˆÙ…Ù‹Ø§ Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø¯ÙˆÙ† Ø§Ø®ØªÙ„Ø§Ù‚."},
                {"role": "user", "content": faq_prompt}
            ]
            try:
                faq_block = chat_complete_cached(
                    client, faq_messages,
                    model=primary_model, fallback_model=fallback_model,
                    temperature=0.4, max_tokens=700,
                    cacher=st.session_state["llm_cacher"],
                    cache_extra={"task":"faq_sources","city":place_name or city_input}
                )
            except Exception:
                faq_block = FAQ_TMPL.format(category=category, city=place_name or city_input)
        else:
            faq_block = "â€”"

        # Ù…Ù†Ù‡Ø¬ÙŠØ©
        last_updated = datetime.now().strftime("%B %Y")
        methodology_block = METH_TMPL.format(last_updated=last_updated) if include_methodology else "â€”"

        # ÙƒØªÙ„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
        req_md = "\n".join([f"- **{kw}** â€” Ø­Ø¯ Ø£Ø¯Ù†Ù‰: {need} Ù…Ø±Ù‘Ø©" for kw, need in required_list]) if required_list else "â€”"

        # Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
        ref_urls = normalize_refs(refs_text)
        references_block = build_references_md(ref_urls) if ref_urls else "â€”"
        citation_map = build_citation_map(ref_urls)

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        base_prompt = BASE_TMPL.format(
            title=article_title, keyword=keyword, content_scope=content_scope, category=category,
            restaurants_list=", ".join(restaurants), criteria_block=criteria_block, faq_block=faq_block,
            methodology_block=methodology_block, tone_label=tone, place_context=place_context,
            protip_hint=protip_hint, scope_instructions=scope_instructions, tone_instructions=tone_instructions,
            tone_selection_line=tone_selection_line.replace("{last_updated}", last_updated),
            required_keywords_block=req_md, approx_len=approx_len,
            references_block=references_block
        )
        base_messages = [
            {"role": "system",
             "content": (
                 f"Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰. {system_tone}. Ø·ÙˆÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ {approx_len} ÙƒÙ„Ù…Ø©."
                 " Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚ ÙˆÙ„Ø§ ØªØ®ØªÙ„Ù‚ Ø­Ù‚Ø§Ø¦Ù‚ Ø£Ùˆ Ù…ØµØ§Ø¯Ø±. Ø¥Ù† Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø© Ø®Ø§Ø±Ø¬ÙŠØ©ØŒ Ù„Ø§ ØªØ°ÙƒØ±Ù‡Ø§."
                 " Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…ØµØ¯Ø± Ø®Ø§Ø±Ø¬ÙŠ Ø§Ø³ØªØ®Ø¯Ù… Ø­Ø§Ø´ÙŠØ© [^n] ÙÙ‚Ø·ØŒ Ø¯ÙˆÙ† ØªÙˆÙ„ÙŠØ¯ Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ØªÙ†."
             )},
            {"role": "user", "content": base_prompt},
        ]

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„
        try:
            article_md = chat_complete_cached(
                client, base_messages,
                max_tokens=2200, temperature=0.7,
                model=primary_model, fallback_model=fallback_model,
                cacher=st.session_state["llm_cacher"],
                cache_extra={"task":"article_base", "required": required_list}
            )
        except Exception as e:
            st.error(f"ÙØ´Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
            st.stop()

        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ù„Ù…Ø³Ø§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        apply_polish = add_human_touch or any(checks.values())
        merged_user_notes = (st.session_state.get("comp_gap_notes","") + "\n" + (manual_notes or "")).strip()
        if apply_polish or merged_user_notes:
            polish_prompt = POLISH_TMPL.format(article=article_md, user_notes=merged_user_notes)
            polish_messages = [
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ Ù…Ø­ØªØ±ÙØŒ ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ ÙˆØªØ¶ÙŠÙ Ù„Ù…Ø³Ø§Øª Ø¨Ø´Ø±ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ©."},
                {"role": "user", "content": polish_prompt},
            ]
            try:
                article_md = chat_complete_cached(
                    client, polish_messages,
                    max_tokens=2400, temperature=0.8,
                    model=primary_model, fallback_model=fallback_model,
                    cacher=st.session_state["llm_cacher"],
                    cache_extra={"task":"polish"}
                )
            except Exception as e:
                st.warning(f"Ø·Ø¨Ù‚Ø© Ø§Ù„Ù„Ù…Ø³Ø§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ØªØ¹Ø°Ù‘Ø±Øª: {e}")

        # ÙØ­Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© + Ø¥ØµÙ„Ø§Ø­ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
        kw_report = enforce_report(article_md, required_list)
        st.subheader("ğŸ§© Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©")
        if not required_list:
            st.caption("Ù„Ù… ØªÙØ­Ø¯Ù‘Ø¯ ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ø²Ø§Ù…ÙŠØ©.")
        else:
            rows = []
            for item in kw_report["items"]:
                status = "âœ…" if item["ok"] else "âŒ"
                rows.append(f"- {status} **{item['keyword']}** â€” Ù…Ø·Ù„ÙˆØ¨ {item['min']}, ÙˆÙØ¬Ø¯Øª {item['found']}")
            st.markdown("\n".join(rows))

            if not kw_report["ok"]:
                needs_lines = "\n".join([f"- {m['keyword']}: Ù†Ø­ØªØ§Ø¬ +{m['need']}" for m in kw_report["missing"]])
                if st.button("âœï¸ Ø¥Ø¯Ù…Ø§Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ù‚ØµØ© (Ø¯ÙˆÙ† Ø­Ø´Ùˆ)"):
                    fix_msgs = [
                        {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ Ø¯Ù‚ÙŠÙ‚ ØªÙØ¯Ø®Ù„ ÙƒÙ„Ù…Ø§Øª Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ù†Ø¹ÙˆÙ…Ø© ÙˆØ¨Ø¯ÙˆÙ† Ø­Ø´Ùˆ."},
                        {"role": "user", "content": FIX_PROMPT.format(orig=article_md[:12000], needs=needs_lines)}
                    ]
                    try:
                        article_md = chat_complete_cached(
                            client, fix_msgs,
                            max_tokens=2400, temperature=0.5,
                            model=primary_model, fallback_model=fallback_model,
                            cacher=st.session_state["llm_cacher"],
                            cache_extra={"task":"kw_fix", "needs": kw_report["missing"]}
                        )
                        kw_report = enforce_report(article_md, required_list)
                        st.success("ØªÙ… Ø¥Ø¯Ù…Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.")
                        st.markdown("\n".join(
                            [f"- {'âœ…' if it['ok'] else 'âŒ'} **{it['keyword']}** â€” Ù…Ø·Ù„ÙˆØ¨ {it['min']}, ÙˆÙØ¬Ø¯Øª {it['found']}"
                             for it in kw_report["items"]]
                        ))
                    except Exception as e:
                        st.warning(f"ØªØ¹Ø°Ù‘Ø± Ø§Ù„Ø¥Ø¯Ù…Ø§Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {e}")

        # Meta
        meta_prompt = f"ØµÙØº Ø¹Ù†ÙˆØ§Ù† SEO (â‰¤ 60) ÙˆÙˆØµÙ Ù…ÙŠØªØ§ (â‰¤ 155) Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ù†ÙˆØ§Ù† \"{article_title}\". Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {keyword}.\nTITLE: ...\nDESCRIPTION: ..."
        try:
            meta_out = chat_complete_cached(
                client,
                [{"role":"system","content":"Ø£Ù†Øª Ù…Ø®ØªØµ SEO Ø¹Ø±Ø¨ÙŠ."},{"role":"user","content": meta_prompt}],
                max_tokens=200, temperature=0.6,
                model=primary_model, fallback_model=fallback_model,
                cacher=st.session_state["llm_cacher"],
                cache_extra={"task":"meta"}
            )
        except Exception:
            meta_out = f"TITLE: {article_title}\nDESCRIPTION: Ø¯Ù„ÙŠÙ„ Ø¹Ù…Ù„ÙŠ Ø¹Ù† {keyword}."

        # Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©
        links_catalog = [s.strip() for s in internal_catalog.splitlines() if s.strip()]
        links_prompt = (
            f"Ø§Ù‚ØªØ±Ø­ 3 Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù† Ø£Ù…ÙƒÙ†:\n{links_catalog}\n"
            f"Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {article_title}\nØ§Ù„Ù†Ø·Ø§Ù‚: {content_scope}\nØ§Ù„ÙØ¦Ø©: {category}\n"
            f"Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©/Ø§Ù„Ù…ÙƒØ§Ù†: {place_name or city_input}\nÙ…Ù‚ØªØ·Ù:\n{article_md[:800]}\n"
            "- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: <Ø§Ù„Ù†Øµ>\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: <Ø§Ù„Ù†Øµ>\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: <Ø§Ù„Ù†Øµ>"
        )
        try:
            links_out = chat_complete_cached(
                client,
                [{"role":"system","content":"Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ ÙŠÙ‚ØªØ±Ø­ Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ©."},{"role":"user","content": links_prompt}],
                max_tokens=240, temperature=0.5,
                model=primary_model, fallback_model=fallback_model,
                cacher=st.session_state["llm_cacher"],
                cache_extra={"task":"internal_links"}
            )
        except Exception:
            links_out = "- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ø±ÙŠØ§Ø¶\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: Ø¯Ù„ÙŠÙ„ Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ø¹Ø§Ø¦Ù„Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†Ù…Ø§Ø·"

        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„Ù†ÙˆØ§ØªØ¬
        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬")
        st.markdown(article_md)
        st.session_state['last_article_md'] = article_md

        st.subheader("ğŸ” Meta (SEO)"); st.code(meta_out, language="text")
        st.subheader("ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©"); st.markdown(links_out)

        # ===== JSON-LD (Article + FAQPage) =====
        jsonld = {
            "@context": "https://schema.org",
            "@graph": [
                {
                    "@type": "Article",
                    "headline": article_title,
                    "inLanguage": "ar",
                    "keywords": [keyword] if keyword else [],
                    "genre": ["Ø¯Ù„ÙŠÙ„ Ù…Ø·Ø§Ø¹Ù…", "Ù…Ø±Ø§Ø¬Ø¹Ø§Øª"],
                    "articleBody": article_md[:5000],
                    "datePublished": datetime.now().strftime("%Y-%m-%d"),
                    "dateModified": datetime.now().strftime("%Y-%m-%d"),
                    "author": {"@type": "Person", "name": author_name} if author_name else {"@type":"Organization","name":"ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ­Ø±ÙŠØ±"},
                    **({"reviewedBy": {"@type": "Person", "name": reviewer_name}} if reviewer_name else {}),
                    "isAccessibleForFree": True,
                    "mainEntityOfPage": {"@type": "WebPage", "name": article_title},
                    "citation": list(citation_map.values()),  # Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
                }
            ]
        }

        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Q/A Ù„Ù„Ù€FAQPage (Ø¨Ø³ÙŠØ·Ø© ÙˆØºÙŠØ± Ù…Ø«Ø§Ù„ÙŠØ©)
        faq_pairs = []
        try:
            import re as _re
            # Ø£Ù†Ù…Ø§Ø· Ø´Ø§Ø¦Ø¹Ø©: "- **Ø³Ø¤Ø§Ù„**\nØ¬ÙˆØ§Ø¨"
            blocks = _re.findall(r"-\s*\*\*(.+?)\*\*\s*\n([^\n].*?)(?=\n- \*\*|\Z)", faq_block, flags=_re.DOTALL)
            for q, a in blocks:
                q = q.strip()
                a = a.strip()
                if q and a:
                    faq_pairs.append({"question": q, "answer": a})
        except Exception:
            pass

        if faq_pairs:
            jsonld["@graph"].append({
                "@type": "FAQPage",
                "inLanguage": "ar",
                "mainEntity": [
                    {"@type": "Question", "name": qa["question"], "acceptedAnswer": {"@type":"Answer","text": qa["answer"]}}
                    for qa in faq_pairs[:12]
                ]
            })

        jsonld_str = json.dumps(jsonld, ensure_ascii=False, indent=2)
        st.subheader("ğŸ§¾ JSON-LD")
        st.code(jsonld_str, language="json")
        st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ JSON-LD", data=jsonld_str, file_name=f"{slugify(article_title)}.json", mime="application/ld+json")

        # Ø­ÙØ¸ JSON ØªÙ„Ø®ÙŠØµÙŠ
        json_obj = {"title": article_title, "keyword": keyword, "category": category,
            "country": country, "city": city_input,
            "place": {"type": place_type, "name": place_name, "rules": place_rules, "strict": strict_in_scope},
            "content_scope": content_scope, "restaurants": restaurants, "last_updated": datetime.now().strftime("%B %Y"),
            "tone": tone, "reviews_weight": review_weight, "models": {"primary": primary_model, "fallback": fallback_model},
            "include_faq": include_faq, "include_methodology": include_methodology,
            "article_markdown": article_md, "meta": meta_out, "internal_links": links_out,
            "references": list(citation_map.values()), "author": author_name, "reviewer": reviewer_name, "last_verified": last_verified}
        st.session_state['last_json'] = to_json(json_obj)

    with col2:
        colA, colB, colC = st.columns(3)
        with colA:
            md_data = st.session_state.get('last_article_md', '')
            st.download_button('ğŸ’¾ ØªÙ†Ø²ÙŠÙ„ Markdown', data=md_data, file_name='article.md', mime='text/markdown')
        with colB:
            md_data = st.session_state.get('last_article_md', '')
            st.download_button('ğŸ“ ØªÙ†Ø²ÙŠÙ„ DOCX', data=to_docx(md_data), file_name='article.docx', mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        with colC:
            json_data = st.session_state.get('last_json', '{}')
            st.download_button('ğŸ§© ØªÙ†Ø²ÙŠÙ„ JSON', data=json_data, file_name='article.json', mime='application/json')

# ------------------ Tab 2: Competitor Analysis ------------------
with tab_comp:
    st.subheader("ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ† â€” Ø±ÙˆØ§Ø¨Ø· ÙŠØ¯ÙˆÙŠØ© (Ø¨Ø¯ÙˆÙ† API)")
    st.markdown("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø·ÙŠÙ† Ù„Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØµØ¯Ù‘Ø±Ø©. Ø³Ù†Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆÙ†Ø­Ù„Ù‘Ù„Ù‡ Ù…Ù† Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆE-E-A-T ÙÙ‚Ø·.")
    query = st.text_input("Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«", "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¯Ø¨ÙŠ Ù…ÙˆÙ„")
    place_scope_desc = st.text_input("ÙˆØµÙ Ø§Ù„Ù†Ø·Ø§Ù‚/Ø§Ù„Ù…ÙƒØ§Ù† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", "Ø¯Ø§Ø®Ù„ Ø¯Ø¨ÙŠ Ù…ÙˆÙ„ ÙÙ‚Ø·")
    url_a = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ A", "")
    url_b = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ B", "")

    tone_for_analysis = st.selectbox("Ù†Ø¨Ø±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„",
        ["Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª", "Ø¯Ù„ÙŠÙ„ ØªØ­Ø±ÙŠØ±ÙŠ Ù…Ø­Ø§ÙŠØ¯"], index=0)
    reviews_weight_analysis = st.slider("ÙˆØ²Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª (Ùª) ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„", 0, 100, 60, step=5)

    colx, coly = st.columns(2)
    with colx: fetch_btn = st.button("ğŸ“¥ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
    with coly: analyze_btn = st.button("ğŸ§  ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„")

    if fetch_btn:
        if not url_a or not url_b:
            st.warning("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø·ÙŠÙ† Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            try:
                with st.spinner("Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© A..."):
                    page_a = fetch_and_extract(url_a)
                with st.spinner("Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© B..."):
                    page_b = fetch_and_extract(url_b)
                st.session_state["comp_pages"] = {"A": page_a, "B": page_b}
                st.success("ØªÙ… Ø§Ù„Ø¬Ù„Ø¨ ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø©.")
                st.write("**A:**", page_a.get("title") or url_a, f"({page_a['word_count']} ÙƒÙ„Ù…Ø©)")
                st.write("**B:**", page_b.get("title") or url_b, f"({page_b['word_count']} ÙƒÙ„Ù…Ø©)")
                st.caption("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„.")
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± Ø§Ù„Ø¬Ù„Ø¨: {e}")

    if analyze_btn:
        if not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
            st.stop()
        pages = st.session_state.get("comp_pages")
        if not pages:
            st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            client = get_client()
            try:
                with st.spinner("ÙŠØ´ØºÙ‘Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
                    analysis_md = analyze_competitors(
                        client, primary_model, fallback_model,
                        pages["A"], pages["B"],
                        query, place_scope_desc or "â€”",
                        tone_for_analysis, reviews_weight_analysis,
                        cacher=st.session_state["llm_cacher"]
                    )
                st.session_state["comp_analysis_md"] = analysis_md
                st.subheader("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„"); st.markdown(analysis_md)
                gaps = extract_gap_points(analysis_md)
                if gaps:
                    st.info("ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØµÙŠØ§Øª Gap-to-Win â€” ÙŠÙ…ÙƒÙ†Ùƒ Ø­Ù‚Ù†Ù‡Ø§ ÙÙŠ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…Ù‚Ø§Ù„.")
                    st.text_area("Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© (Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­Ø±ÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„Ø­Ù‚Ù†)", gaps, key="comp_gap_notes", height=160)
                else:
                    st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø³Ù… 'Gap-to-Win'. Ø§Ù†Ø³Ø®Ù‡ ÙŠØ¯ÙˆÙŠÙ‹Ø§.")
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")

# ------------------ Tab 3: QC ------------------
with tab_qc:
    st.subheader("ğŸ§ª ÙØ­Øµ Ø¨Ø´Ø±ÙŠØ© ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
    qc_text = st.text_area("Ø§Ù„ØµÙ‚ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§", st.session_state.get("last_article_md",""), height=300)
    col_q1, col_q2, col_q3 = st.columns(3)
    with col_q1:
        do_fluff = st.checkbox("ÙƒØ´Ù Ø§Ù„Ø­Ø´Ùˆ ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù„Ø¨ÙŠØ©", value=True)
    with col_q2:
        do_eeat = st.checkbox("Ù…Ø¤Ø´Ø±Ø§Øª E-E-A-T", value=True)
    with col_q3:
        do_llm_review = st.checkbox("ØªØ´Ø®ÙŠØµ Ù…ÙØ±Ø´Ø¯ (LLM)", value=True)

    if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹"):
        if not qc_text.strip():
            st.warning("Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            rep = quality_report(qc_text)
            st.session_state["qc_report"] = rep

            # Ø¨Ø·Ø§Ù‚Ø© Ø¯Ø±Ø¬Ø§Øª Ù…ÙÙˆØ³Ù‘Ø¹Ø©
            st.markdown("### Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø¯Ø±Ø¬Ø§Øª")
            c1, c2, c3, c4 = st.columns(4)
            with c1: st.metric("Human-style", rep["human_style_score"])
            with c2: st.metric("Sensory %", rep["sensory_ratio"])
            with c3: st.metric("TTR", rep["ttr"])
            with c4: st.metric("Passive %", rep["passive_ratio"])

            st.markdown("#### Ø¨Ù†ÙŠØ© Ø§Ù„Ù†Øµ")
            colA, colB, colC = st.columns(3)
            with colA:
                st.write(f"- ÙƒÙ„Ù…Ø§Øª: **{rep['word_count']}**")
                st.write(f"- Ø¬ÙÙ…Ù„: **{rep['sentence_count']}**")
                st.write(f"- ÙÙ‚Ø±Ø§Øª: **{rep['paragraph_count']}**")
            with colB:
                st.write(f"- Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©: **{rep['avg_sentence_length']}**")
                st.write(f"- Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„ÙÙ‚Ø±Ø©: **{rep['paragraph_metrics']['avg_len']}** Â± {rep['paragraph_metrics']['std_len']}")
            with colC:
                st.write(f"- ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø©(<20): **{rep['paragraph_metrics']['pct_short_lt20w']}%**")
                st.write(f"- ÙÙ‚Ø±Ø§Øª Ø·ÙˆÙŠÙ„Ø©(>100): **{rep['paragraph_metrics']['pct_long_gt100w']}%**")

            st.markdown("#### ØªÙ†ÙˆÙ‘Ø¹ Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ø¬Ù…Ù„")
            st.json({"top_starts": rep["sentence_variety"]["start_top"], "start_hhi": rep["sentence_variety"]["start_hhi"]})

            st.markdown("#### E-E-A-T & Information Gain")
            m1, m2, m3 = st.columns(3)
            with m1: st.metric("E-E-A-T", rep["eeat_score"])
            with m2: st.metric("Info Gain", rep["info_gain_score"])
            with m3: st.metric("Fluff Density", rep["fluff_density"])
            st.json(rep["eeat"])

            if do_fluff:
                st.markdown("#### ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª (N-grams)")
                reps = rep.get("repeated_phrases") or []
                if reps:
                    for g, c in reps:
                        st.write(f"- `{g}` Ã— {c}")
                else:
                    st.caption("Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø± Ù…Ø²Ø¹Ø¬ Ù…Ù„Ø­ÙˆØ¸.")

                st.markdown("#### Ø¹Ø¨Ø§Ø±Ø§Øª Ù‚Ø§Ù„Ø¨ÙŠØ© Ù…Ø±ØµÙˆØ¯Ø©")
                boiler = rep.get("boilerplate_flags") or []
                if boiler:
                    for f in boiler:
                        st.write(f"- **Ù†Ù…Ø·:** `{f['pattern']}` â€” â€¦{f['excerpt']}â€¦")
                else:
                    st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ø¨Ø§Ø±Ø§Øª Ù‚Ø§Ù„Ø¨ÙŠØ© Ø¸Ø§Ù‡Ø±Ø©.")

            st.markdown("#### Ø§Ù„Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ")
            st.json(rep["sentiment"])

            if do_eeat:
                st.markdown("#### Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ£Ù‚Ø³Ø§Ù…")
                st.json(rep["headings"])

            st.markdown("#### ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ©")
            for tip in rep["tips"]:
                st.write(f"- {tip}")

            st.success("Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹.")
            st.session_state["qc_text"] = qc_text

    if do_llm_review and st.button("ğŸ§  ØªØ´Ø®ÙŠØµ Ù…ÙØ±Ø´Ø¯ (LLM)"):
        if not qc_text.strip():
            st.warning("Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ù‹Ø§.")
        elif not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
        else:
            client = get_client()
            out = llm_review(client, primary_model, fallback_model, qc_text, cacher=st.session_state["llm_cacher"])
            st.markdown("### ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØ±Ø§Ø¬Ø¹"); st.markdown(out)
            st.session_state["qc_review_md"] = out

    st.markdown("---")
    st.markdown("#### Ø¥ØµÙ„Ø§Ø­ Ø°ÙƒÙŠ Ù„Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù„Ù‘Ù…Ø©")
    flagged_block = st.text_area("Ø£Ù„ØµÙ‚ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ø³ÙŠÙ†Ù‡Ø§ (Ø³Ø·Ø± Ù„ÙƒÙ„ Ù…Ù‚Ø·Ø¹)", height=140, placeholder="Ø§Ù†Ø³Ø® Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø¶Ø¹ÙŠÙØ© ÙˆØ¶Ø¹Ù‡Ø§ Ù‡Ù†Ø§â€¦")
    if st.button("âœï¸ Ø£Ø¹ÙØ¯ Ø§Ù„ØµÙŠØ§ØºØ© Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø·"):
        if not flagged_block.strip():
            st.warning("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø£ÙˆÙ„Ù‹Ø§.")
        elif not qc_text.strip():
            st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ø£Ø³Ø§Ø³ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©.")
        elif not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
        else:
            client = get_client()
            new_text = llm_fix(client, primary_model, fallback_model, qc_text, flagged_block.splitlines(), cacher=st.session_state["llm_cacher"])
            st.markdown("### Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ØµÙ„Ø§Ø­"); st.markdown(new_text)
            st.session_state["last_article_md"] = new_text
            st.success("ØªÙ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ.")
